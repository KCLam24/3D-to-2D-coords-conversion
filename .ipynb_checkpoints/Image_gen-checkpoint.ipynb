{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cb7b8f-3d40-4ade-bce2-fea128ee0a1c",
   "metadata": {},
   "source": [
    "# Generate images and save as .npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8572f6-e552-41f1-bc7f-bbebd06fd634",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7955f9f5-e21c-4b7b-a61c-4a1dc42df79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Model: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n"
     ]
    }
   ],
   "source": [
    "#creating responsive plot\n",
    "%matplotlib widget\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "import platform\n",
    "from scipy.stats import multivariate_normal, lognorm\n",
    "\n",
    "cpu_info = platform.processor()\n",
    "print(f\"CPU Model: {cpu_info}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee427d2c-6bfa-4125-8708-abb9baa7054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.00 GB\n",
      "Used GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212bd1-496c-4bad-a83e-fe02c59caa3b",
   "metadata": {},
   "source": [
    "## Functions required for image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767594bb-2667-4d22-b9b5-765f6464ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9702341897615874 1.455204250749316\n",
      "0.8968844939766708 1.5714511428320712\n",
      "0.5840081677421798 1.1494453092552204\n",
      "1.424707412403306 0.5557051001202231\n",
      "1.0038197576633017 0.5110212619753602\n",
      "0.7197100526108691 0.9683094314383589\n",
      "1.9281022145421636 0.5600797983742578\n",
      "-0.20431217667557444 0.18243470669503736\n",
      "0.4987889001536506 1.292971494259562\n",
      "0.7858404159030603 0.5273605734162262\n"
     ]
    }
   ],
   "source": [
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=1).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "\n",
    "mean = [1, 1]  # Mean for a/c and b/c ratios\n",
    "variance_a = 0.35  # Variance for a/c\n",
    "variance_b = 0.2  # Variance for b/c\n",
    "cov_ab = 0.05  # Covariance between a/c and b/c\n",
    "\n",
    "# Covariance matrix for the bivariate normal distribution\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "for _ in range(10):\n",
    "    a, b = generate_ratios(mean, cov, False)\n",
    "    print(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0481db64-1021-4560-9c8a-91cb3ca3fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=1).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "# Generate uniform points on surface of sphere for normal projections\n",
    "def projection_points(radius, num_points):\n",
    "    \n",
    "    phi = np.linspace(0, np.pi, num_points)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "    theta, phi = np.meshgrid(theta, phi)\n",
    "    \n",
    "    x = radius * np.sin(phi) * np.cos(theta)\n",
    "    y = radius * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * np.cos(phi)\n",
    "    \n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, log_normal=False, plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, log_normal)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = projection_points(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10,20)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, log_normal=False)\n",
    "            #a = ellipsoid(max_ellipsoid_size, no_of_points) # create ellipsoid\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10, 20)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7cedbb8-c41a-452a-93af-e6ee90dfbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9df01f8b-58a9-423d-be0b-74b9b2bc3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model being used is SimpleCNN\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3be456b1-318a-4045-9821-2a3481cd6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance a:0.1, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 587 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6802 | Train acc: 0.5833 | Test loss: 0.7885 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6792 | Train acc: 0.5833 | Test loss: 0.7878 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 505 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6852 | Train acc: 0.5833 | Test loss: 0.7836 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6835 | Train acc: 0.5833 | Test loss: 0.7835 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 691 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6877 | Train acc: 0.5833 | Test loss: 0.8025 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6862 | Train acc: 0.5833 | Test loss: 0.8022 | Test acc: 0.2500\n",
      "Total training time: 0.026 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 881 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6768 | Train acc: 0.5833 | Test loss: 0.7931 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6747 | Train acc: 0.5833 | Test loss: 0.7906 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 585 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6816 | Train acc: 0.5833 | Test loss: 0.7856 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6803 | Train acc: 0.5833 | Test loss: 0.7866 | Test acc: 0.2500\n",
      "Total training time: 0.022 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 505 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6734 | Train acc: 0.5833 | Test loss: 0.7985 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6720 | Train acc: 0.5833 | Test loss: 0.7986 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 515 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6645 | Train acc: 0.5833 | Test loss: 0.7986 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6634 | Train acc: 0.5833 | Test loss: 0.7988 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 862 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6731 | Train acc: 0.5833 | Test loss: 0.7920 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6714 | Train acc: 0.5833 | Test loss: 0.7894 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.1\n",
      "System generation finished in 0 minutes 0 seconds 362 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6856 | Train acc: 0.5833 | Test loss: 0.7901 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6845 | Train acc: 0.5833 | Test loss: 0.7901 | Test acc: 0.2500\n",
      "Total training time: 0.022 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.45\n",
      "System generation finished in 0 minutes 0 seconds 468 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6717 | Train acc: 0.5833 | Test loss: 0.7900 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6704 | Train acc: 0.5833 | Test loss: 0.7880 | Test acc: 0.2500\n",
      "Total training time: 0.026 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 640 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6805 | Train acc: 0.5833 | Test loss: 0.7843 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6793 | Train acc: 0.5833 | Test loss: 0.7806 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 697 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6774 | Train acc: 0.5833 | Test loss: 0.7999 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6756 | Train acc: 0.5833 | Test loss: 0.7963 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 511 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6786 | Train acc: 0.5833 | Test loss: 0.7751 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6770 | Train acc: 0.5833 | Test loss: 0.7774 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 627 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6706 | Train acc: 0.5833 | Test loss: 0.7881 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6695 | Train acc: 0.5833 | Test loss: 0.7878 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 509 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6792 | Train acc: 0.5833 | Test loss: 0.7989 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6777 | Train acc: 0.5833 | Test loss: 0.7968 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 589 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6735 | Train acc: 0.5833 | Test loss: 0.7854 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6723 | Train acc: 0.5833 | Test loss: 0.7879 | Test acc: 0.2500\n",
      "Total training time: 0.038 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = b = [0.1, 0.45, 0.7, 0.87]\n",
    "mean = [1, 1]  # Mean for a/c and b/c ratios\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(b)):\n",
    "        variance_a = a[i]  # Variance for a/c\n",
    "        variance_b = b[j]  # Variance for b/c\n",
    "        # Covariance matrix for the bivariate normal distribution\n",
    "        cov = [[variance_a, cov_ab], \n",
    "               [cov_ab, variance_b]]\n",
    "        start = time.time()\n",
    "        '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "        \n",
    "        sphere_img2 = system_maker(2,30,1,200,2,(64,64))\n",
    "        ellips_img2 = system_maker2(2,30,1,200,2,(64,64))\n",
    "        end = time.time()\n",
    "        \n",
    "        #images_bw = np.concatenate((sphere_img, ellips_img), axis=0)\n",
    "        images_cont = np.concatenate((sphere_img2, ellips_img2), axis=0)\n",
    "        \n",
    "        # record time\n",
    "        creation_time = end - start\n",
    "        \n",
    "        label = label_making(0, sphere_img)\n",
    "        label2 = label_making(1, ellips_img)\n",
    "        label.extend(label2)\n",
    "        labels_array = np.array(label)\n",
    "        \n",
    "        #np.savez(f'images_{i}{j}',images_cont=images_cont, images_bw=images_bw, labels=labels_array)\n",
    "        plot_images(images_cont[4],images_cont[0],images_cont[14],images_cont[11],f'image_a{i}_b{j}')\n",
    "        print(f'variance a:{variance_a}, variance b:{variance_b}')\n",
    "        print(f'System generation finished in {creation_time // 60:.0f} minutes {creation_time % 60:.0f} seconds {creation_time % 1 * 1000:.0f} milliseconds')\n",
    "\n",
    "        #Data transformation\n",
    "        data_array = np.array(images_cont)\n",
    "    \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_array, labels_array, test_size=0.2, random_state=42)\n",
    "        #print(f\"Training data shape: {X_train.shape}\")\n",
    "        #print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        # Instantiate the dataset\n",
    "        BATCH_SIZE = 32\n",
    "        NUM_WORKERS = 0 #os.cpu_count()\n",
    "        \n",
    "        train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "        \n",
    "        #print(f'Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...')\n",
    "\n",
    "        # Trainig begins\n",
    "        # Set no of epochs (newnet)\n",
    "        NUM_EPOCHS = 5\n",
    "        \n",
    "        # Setup loss function and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # Train model\n",
    "        model_results = train(model=model,\n",
    "                             train_data=train_dataloader,\n",
    "                             test_data=val_dataloader,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_fn=loss_fn,\n",
    "                             epochs=NUM_EPOCHS)\n",
    "        # End timer and print out time taken\n",
    "        end_time = timer()\n",
    "        print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "        \n",
    "        # Calculate parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f'Total parameters: {total_params}\\n')\n",
    "        plot_loss_curves(model_results, f'Loss_a{i}_b{j}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f13084-a689-41ab-bf99-b93e02c0c06a",
   "metadata": {},
   "source": [
    "mean = [1, 1]  # Mean for a/c and b/c ratios\n",
    "variance_a = 0.1  # Variance for a/c\n",
    "variance_b = 0.1  # Variance for b/c\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "\n",
    "# Covariance matrix for the bivariate normal distribution\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "'''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "\n",
    "sphere_img2 = system_maker(2,30,1,200,2,(64,64))\n",
    "ellips_img2 = system_maker2(2,30,1,200,2,(64,64))\n",
    "end = time.time()\n",
    "\n",
    "#images_bw = np.concatenate((sphere_img, ellips_img), axis=0)\n",
    "images_cont = np.concatenate((sphere_img2, ellips_img2), axis=0)\n",
    "\n",
    "# record time\n",
    "creation_time = end - start\n",
    "\n",
    "label = label_making(0, sphere_img)\n",
    "label2 = label_making(1, ellips_img)\n",
    "label.extend(label2)\n",
    "labels_array = np.array(label)\n",
    "\n",
    "np.savez('images',images_cont=images_cont, images_bw=images_bw, labels=labels_array)\n",
    "\n",
    "print(f'System generation finished in {creation_time // 60:.0f} minutes {creation_time % 60:.0f} seconds {creation_time % 1 * 1000:.0f} milliseconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0201a9-4677-46fe-8392-a26b54ddafc7",
   "metadata": {},
   "source": [
    "# Qsub script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7bb9b5-fb08-4f1c-bbde-927e67758fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.00 GB\n",
      "Used GPU Memory: 0.00 GB\n",
      "Model being used is SimpleCNN\n",
      "variance a:0.3, variance b:0.3, mean:[1, 1]\n",
      "System generation finished in 0 minutes 13 seconds 337 milliseconds\n",
      "Training data shape: (1280, 64, 64)\n",
      "Validation data shape: (320, 64, 64)\n",
      "Epoch: 1 | Train loss: 0.7040 | Train acc: 0.5102 | Test loss: 0.6819 | Test acc: 0.5344\n",
      "Epoch: 2 | Train loss: 0.7055 | Train acc: 0.5211 | Test loss: 0.6720 | Test acc: 0.6406\n",
      "Epoch: 3 | Train loss: 0.6650 | Train acc: 0.5992 | Test loss: 0.6676 | Test acc: 0.5344\n",
      "Epoch: 4 | Train loss: 0.6637 | Train acc: 0.5711 | Test loss: 0.6492 | Test acc: 0.6156\n",
      "Epoch: 5 | Train loss: 0.6491 | Train acc: 0.6125 | Test loss: 0.7202 | Test acc: 0.4906\n",
      "Epoch: 6 | Train loss: 0.6365 | Train acc: 0.6492 | Test loss: 0.6342 | Test acc: 0.6656\n",
      "Epoch: 7 | Train loss: 0.6270 | Train acc: 0.6438 | Test loss: 0.6229 | Test acc: 0.6250\n",
      "Epoch: 8 | Train loss: 0.6050 | Train acc: 0.6758 | Test loss: 0.6148 | Test acc: 0.6344\n",
      "Epoch: 9 | Train loss: 0.5994 | Train acc: 0.7008 | Test loss: 0.6150 | Test acc: 0.6625\n",
      "Epoch: 10 | Train loss: 0.5866 | Train acc: 0.6898 | Test loss: 0.6019 | Test acc: 0.6469\n",
      "Epoch: 11 | Train loss: 0.5869 | Train acc: 0.6953 | Test loss: 0.5985 | Test acc: 0.6531\n",
      "Epoch: 12 | Train loss: 0.5758 | Train acc: 0.7016 | Test loss: 0.5982 | Test acc: 0.6406\n",
      "Epoch: 13 | Train loss: 0.5616 | Train acc: 0.7188 | Test loss: 0.5878 | Test acc: 0.6656\n",
      "Epoch: 14 | Train loss: 0.5544 | Train acc: 0.7141 | Test loss: 0.6290 | Test acc: 0.6625\n",
      "Epoch: 15 | Train loss: 0.5550 | Train acc: 0.7125 | Test loss: 0.6158 | Test acc: 0.6719\n",
      "Epoch: 16 | Train loss: 0.5554 | Train acc: 0.7086 | Test loss: 0.5752 | Test acc: 0.6750\n",
      "Epoch: 17 | Train loss: 0.5492 | Train acc: 0.7211 | Test loss: 0.5800 | Test acc: 0.6875\n",
      "Epoch: 18 | Train loss: 0.5270 | Train acc: 0.7406 | Test loss: 0.5794 | Test acc: 0.6875\n",
      "Epoch: 19 | Train loss: 0.5304 | Train acc: 0.7281 | Test loss: 0.5692 | Test acc: 0.6937\n",
      "Epoch: 20 | Train loss: 0.5305 | Train acc: 0.7484 | Test loss: 0.5617 | Test acc: 0.7125\n",
      "Total training time: 2.685 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean:[1, 1]\n",
      "System generation finished in 0 minutes 14 seconds 5 milliseconds\n",
      "Training data shape: (1280, 64, 64)\n",
      "Validation data shape: (320, 64, 64)\n",
      "Epoch: 1 | Train loss: 0.7375 | Train acc: 0.5375 | Test loss: 0.6594 | Test acc: 0.6000\n",
      "Epoch: 2 | Train loss: 0.6821 | Train acc: 0.5578 | Test loss: 0.6744 | Test acc: 0.5625\n",
      "Epoch: 3 | Train loss: 0.6598 | Train acc: 0.6031 | Test loss: 0.6701 | Test acc: 0.5719\n",
      "Epoch: 4 | Train loss: 0.6744 | Train acc: 0.5711 | Test loss: 0.6637 | Test acc: 0.6125\n",
      "Epoch: 5 | Train loss: 0.6770 | Train acc: 0.5844 | Test loss: 0.7649 | Test acc: 0.4938\n",
      "Epoch: 6 | Train loss: 0.6710 | Train acc: 0.5742 | Test loss: 0.6525 | Test acc: 0.6031\n",
      "Epoch: 7 | Train loss: 0.6403 | Train acc: 0.6305 | Test loss: 0.6909 | Test acc: 0.5500\n",
      "Epoch: 8 | Train loss: 0.6425 | Train acc: 0.6359 | Test loss: 0.6671 | Test acc: 0.5750\n",
      "Epoch: 9 | Train loss: 0.6477 | Train acc: 0.6062 | Test loss: 0.6460 | Test acc: 0.6281\n",
      "Epoch: 10 | Train loss: 0.6230 | Train acc: 0.6602 | Test loss: 0.6477 | Test acc: 0.6281\n",
      "Epoch: 11 | Train loss: 0.6170 | Train acc: 0.6656 | Test loss: 0.7067 | Test acc: 0.5281\n",
      "Epoch: 12 | Train loss: 0.6151 | Train acc: 0.6766 | Test loss: 0.6458 | Test acc: 0.6312\n",
      "Epoch: 13 | Train loss: 0.6109 | Train acc: 0.6797 | Test loss: 0.6761 | Test acc: 0.5625\n",
      "Epoch: 14 | Train loss: 0.6149 | Train acc: 0.6578 | Test loss: 0.6347 | Test acc: 0.6344\n",
      "Epoch: 15 | Train loss: 0.5923 | Train acc: 0.6937 | Test loss: 0.6305 | Test acc: 0.6312\n",
      "Epoch: 16 | Train loss: 0.5980 | Train acc: 0.6797 | Test loss: 0.6267 | Test acc: 0.6469\n",
      "Epoch: 17 | Train loss: 0.5770 | Train acc: 0.7273 | Test loss: 0.6235 | Test acc: 0.6469\n",
      "Epoch: 18 | Train loss: 0.5718 | Train acc: 0.7250 | Test loss: 0.6372 | Test acc: 0.6312\n",
      "Epoch: 19 | Train loss: 0.5632 | Train acc: 0.7328 | Test loss: 0.6381 | Test acc: 0.6156\n",
      "Epoch: 20 | Train loss: 0.5613 | Train acc: 0.7117 | Test loss: 0.6608 | Test acc: 0.5969\n",
      "Total training time: 2.783 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean:[1, 1]\n",
      "System generation finished in 0 minutes 13 seconds 496 milliseconds\n",
      "Training data shape: (1280, 64, 64)\n",
      "Validation data shape: (320, 64, 64)\n",
      "Epoch: 1 | Train loss: 0.7051 | Train acc: 0.5352 | Test loss: 0.6911 | Test acc: 0.5344\n",
      "Epoch: 2 | Train loss: 0.6861 | Train acc: 0.5563 | Test loss: 0.6716 | Test acc: 0.5719\n",
      "Epoch: 3 | Train loss: 0.6740 | Train acc: 0.5758 | Test loss: 0.6696 | Test acc: 0.5813\n",
      "Epoch: 4 | Train loss: 0.6771 | Train acc: 0.5742 | Test loss: 0.6685 | Test acc: 0.5781\n",
      "Epoch: 5 | Train loss: 0.6655 | Train acc: 0.5922 | Test loss: 0.7062 | Test acc: 0.5125\n",
      "Epoch: 6 | Train loss: 0.6730 | Train acc: 0.5891 | Test loss: 0.6848 | Test acc: 0.5500\n",
      "Epoch: 7 | Train loss: 0.6622 | Train acc: 0.6070 | Test loss: 0.6757 | Test acc: 0.5563\n",
      "Epoch: 8 | Train loss: 0.6568 | Train acc: 0.6141 | Test loss: 0.6773 | Test acc: 0.5625\n",
      "Epoch: 9 | Train loss: 0.6634 | Train acc: 0.6008 | Test loss: 0.6641 | Test acc: 0.6000\n",
      "Epoch: 10 | Train loss: 0.6572 | Train acc: 0.6086 | Test loss: 0.6674 | Test acc: 0.5844\n",
      "Epoch: 11 | Train loss: 0.6580 | Train acc: 0.5984 | Test loss: 0.6704 | Test acc: 0.5906\n",
      "Epoch: 12 | Train loss: 0.6551 | Train acc: 0.6117 | Test loss: 0.6732 | Test acc: 0.5781\n",
      "Epoch: 13 | Train loss: 0.6450 | Train acc: 0.6344 | Test loss: 0.6671 | Test acc: 0.5813\n",
      "Epoch: 14 | Train loss: 0.6424 | Train acc: 0.6508 | Test loss: 0.6672 | Test acc: 0.5781\n",
      "Epoch: 15 | Train loss: 0.6386 | Train acc: 0.6516 | Test loss: 0.6761 | Test acc: 0.5844\n",
      "Epoch: 16 | Train loss: 0.6378 | Train acc: 0.6516 | Test loss: 0.6724 | Test acc: 0.5813\n",
      "Epoch: 17 | Train loss: 0.6344 | Train acc: 0.6742 | Test loss: 0.6727 | Test acc: 0.5813\n",
      "Epoch: 18 | Train loss: 0.6314 | Train acc: 0.6617 | Test loss: 0.6614 | Test acc: 0.5938\n",
      "Epoch: 19 | Train loss: 0.6325 | Train acc: 0.6438 | Test loss: 0.6800 | Test acc: 0.5656\n",
      "Epoch: 20 | Train loss: 0.6318 | Train acc: 0.6664 | Test loss: 0.6699 | Test acc: 0.5750\n",
      "Total training time: 2.750 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Change Varience, same mean at 1,1, not log normal\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "#import platform\n",
    "from scipy.stats import multivariate_normal, lognorm\n",
    "import os \n",
    "\n",
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\t\n",
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=1).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "# Generate uniform points on surface of sphere for normal projections\n",
    "def projection_points(radius, num_points):\n",
    "    \n",
    "    phi = np.linspace(0, np.pi, num_points)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "    theta, phi = np.meshgrid(theta, phi)\n",
    "    \n",
    "    x = radius * np.sin(phi) * np.cos(theta)\n",
    "    y = radius * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * np.cos(phi)\n",
    "    \n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, log_normal=False, plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, log_normal)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = sphere(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res, distance):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10,distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res, distance):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, log_normal=False)\n",
    "            #a = ellipsoid(max_ellipsoid_size, no_of_points) # create ellipsoid\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10, distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\t\n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\t\t\n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\t\t\n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n",
    "\t\t\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "variance_a = 0.3  # Variance for a/c\n",
    "variance_b = 0.3  # Variance for b/c\n",
    "mean = [1, 1]\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "distances = [11, 55, 120]\n",
    "\n",
    "for d in distances:\n",
    "    #start = time.time()\n",
    "    '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "    \n",
    "    sphere_img2 = system_maker(10000,30,1,100,2,(64,64),d)\n",
    "    ellips_img2 = system_maker2(10000,30,1,100,2,(64,64),d)\n",
    "    #end = time.time()\n",
    "    \n",
    "    #images_bw = np.concatenate((sphere_img, ellips_img), axis=0)\n",
    "    images_cont = np.concatenate((sphere_img2, ellips_img2), axis=0)\n",
    "    \n",
    "    # record time\n",
    "    #creation_time = end - start\n",
    "    \n",
    "    label = label_making(0, sphere_img2)\n",
    "    label2 = label_making(1, ellips_img2)\n",
    "    label.extend(label2)\n",
    "    labels_array = np.array(label)\n",
    "    \n",
    "    #np.savez(f'images_{i}{j}',images_cont=images_cont, images_bw=images_bw, labels=labels_array)\n",
    "    plot_images(images_cont[40],images_cont[10050],images_cont[20599],images_cont[34567],f'image_dens_{d}')\n",
    "    print(f'variance a:{variance_a}, variance b:{variance_b}, mean:{mean}, distance: {d}')\n",
    "    print(f'System generation finished in {creation_time // 60:.0f} minutes {creation_time % 60:.0f} seconds {creation_time % 1 * 1000:.0f} milliseconds')\n",
    "    \n",
    "    #Data transformation\n",
    "    data_array = np.array(images_cont)\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data_array, labels_array, test_size=0.2, random_state=42)\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    # Instantiate the dataset\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 0 #os.cpu_count()\n",
    "    \n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "    \n",
    "    #print(f'Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...')\n",
    "    \n",
    "    # Trainig begins\n",
    "    # Set no of epochs (newnet)\n",
    "    NUM_EPOCHS = 200\n",
    "    \n",
    "    # Setup loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Train model\n",
    "    model_results = train(model=model,\n",
    "                         train_data=train_dataloader,\n",
    "                         test_data=val_dataloader,\n",
    "                         optimizer=optimizer,\n",
    "                         loss_fn=loss_fn,\n",
    "                         epochs=NUM_EPOCHS)\n",
    "    # End timer and print out time taken\n",
    "    end_time = timer()\n",
    "    print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "    \n",
    "    # Calculate parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total parameters: {total_params}\\n')\n",
    "    plot_loss_curves(model_results, f'Loss_dens_{d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8f3373-bd74-41d4-85f1-184d675e8f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(images_cont[1599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2a598-6dca-4fc9-859c-cbd4db7ee370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
