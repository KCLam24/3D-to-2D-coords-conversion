{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cb7b8f-3d40-4ade-bce2-fea128ee0a1c",
   "metadata": {},
   "source": [
    "# Generate images and save as .npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8572f6-e552-41f1-bc7f-bbebd06fd634",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7955f9f5-e21c-4b7b-a61c-4a1dc42df79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 15) (4024434401.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"CPU Model: {cpu_info}\\\")\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 15)\n"
     ]
    }
   ],
   "source": [
    "#creating responsive plot\n",
    "%matplotlib widget\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "import platform\n",
    "from scipy.stats import multivariate_normal, lognorm\n",
    "\n",
    "cpu_info = platform.processor()\n",
    "print(f\"CPU Model: {cpu_info}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee427d2c-6bfa-4125-8708-abb9baa7054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.00 GB\n",
      "Used GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212bd1-496c-4bad-a83e-fe02c59caa3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions required for image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37f61d7b-ef12-415b-b8cf-59719fe413dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu = 1          # Mean of the underlying normal distribution\n",
    "sigma = 0.5   # Standard deviation of the underlying normal distribution (sqrt(variance))\n",
    "\n",
    "# Generate random samples\n",
    "samples = lognorm.rvs(s=sigma, scale=np.exp(mu), size=500)\n",
    "\n",
    "# Plot histogram of the samples\n",
    "plt.hist(samples, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Overlay the theoretical PDF\n",
    "x = np.linspace(min(samples), max(samples), 1000)\n",
    "pdf = lognorm.pdf(x, s=sigma, scale=np.exp(mu))\n",
    "plt.plot(x, pdf, 'k', linewidth=2)\n",
    "\n",
    "plt.title('Histogram of Lognormal Distribution Samples')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "767594bb-2667-4d22-b9b5-765f6464ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7598049084917425 1.0326413324454133\n",
      "1.454744682249549 1.3473664370892526\n",
      "2.0507786139325646 1.7317606971297748\n",
      "0.8663633194920097 1.0218972754130669\n",
      "0.5721504354899751 1.9974503640597079\n",
      "1.0393551863246648 1.3087372918806954\n",
      "1.2795192545622227 0.9348244484884778\n",
      "0.744835791229354 0.9236167651938983\n",
      "1.0185217602258807 1.4485612364326053\n",
      "1.707940251556238 0.9147423922976095\n",
      "1.1230783376338231 0.9435792117087494\n",
      "1.317504221896147 1.2955291254167132\n",
      "1.125303362182173 0.7594818087732559\n",
      "1.5123104740681206 0.9638006017291346\n",
      "1.341166300151581 1.8479956523295857\n",
      "0.4228756852288438 1.4149320807090988\n",
      "1.347066280248542 0.9082353104366575\n",
      "0.6012021934613274 2.0582584501154146\n",
      "1.3665353546919599 1.2311034488728978\n",
      "2.159768035654497 0.8076552524932192\n"
     ]
    }
   ],
   "source": [
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=2).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "\n",
    "mean = [0.1, 0.1]  # Mean for a/c and b/c ratios\n",
    "variance_a = 0.2  # Variance for a/c\n",
    "variance_b = 0.1  # Variance for b/c\n",
    "cov_ab = 0.00  # Covariance between a/c and b/c\n",
    "\n",
    "# Covariance matrix for the bivariate normal distribution\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "for _ in range(20):\n",
    "    a, b = generate_ratios(mean, cov, True)\n",
    "    print(a,b)\n",
    "\n",
    "\n",
    "# samples = multivariate_normal.rvs(mean=mean, cov=cov, size=500)\n",
    "\n",
    "# # Plot the samples\n",
    "# plt.scatter(samples[:, 0], samples[:, 1], marker='o', alpha=0.5)\n",
    "# plt.title('Scatter Plot of Multivariate Normal Samples')\n",
    "# plt.xlabel('X1')\n",
    "# plt.ylabel('X2')\n",
    "# plt.axis('equal')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0481db64-1021-4560-9c8a-91cb3ca3fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=2).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "# Generate uniform points on surface of sphere for normal projections\n",
    "def projection_points(radius, num_points):\n",
    "    \n",
    "    phi = np.linspace(0, np.pi, num_points)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "    theta, phi = np.meshgrid(theta, phi)\n",
    "    \n",
    "    x = radius * np.sin(phi) * np.cos(theta)\n",
    "    y = radius * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * np.cos(phi)\n",
    "    \n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, log_normal=False, plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, log_normal)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = projection_points(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10,20)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, log_normal=False)\n",
    "            #a = ellipsoid(max_ellipsoid_size, no_of_points) # create ellipsoid\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(10, 20)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7cedbb8-c41a-452a-93af-e6ee90dfbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9df01f8b-58a9-423d-be0b-74b9b2bc3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model being used is SimpleCNN\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3be456b1-318a-4045-9821-2a3481cd6d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance a:0.1, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 587 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6802 | Train acc: 0.5833 | Test loss: 0.7885 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6792 | Train acc: 0.5833 | Test loss: 0.7878 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 505 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6852 | Train acc: 0.5833 | Test loss: 0.7836 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6835 | Train acc: 0.5833 | Test loss: 0.7835 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 691 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6877 | Train acc: 0.5833 | Test loss: 0.8025 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6862 | Train acc: 0.5833 | Test loss: 0.8022 | Test acc: 0.2500\n",
      "Total training time: 0.026 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.1, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 881 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6768 | Train acc: 0.5833 | Test loss: 0.7931 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6747 | Train acc: 0.5833 | Test loss: 0.7906 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 585 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6816 | Train acc: 0.5833 | Test loss: 0.7856 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6803 | Train acc: 0.5833 | Test loss: 0.7866 | Test acc: 0.2500\n",
      "Total training time: 0.022 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 505 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6734 | Train acc: 0.5833 | Test loss: 0.7985 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6720 | Train acc: 0.5833 | Test loss: 0.7986 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 515 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6645 | Train acc: 0.5833 | Test loss: 0.7986 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6634 | Train acc: 0.5833 | Test loss: 0.7988 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.45, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 862 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6731 | Train acc: 0.5833 | Test loss: 0.7920 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6714 | Train acc: 0.5833 | Test loss: 0.7894 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.1\n",
      "System generation finished in 0 minutes 0 seconds 362 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6856 | Train acc: 0.5833 | Test loss: 0.7901 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6845 | Train acc: 0.5833 | Test loss: 0.7901 | Test acc: 0.2500\n",
      "Total training time: 0.022 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.45\n",
      "System generation finished in 0 minutes 0 seconds 468 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6717 | Train acc: 0.5833 | Test loss: 0.7900 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6704 | Train acc: 0.5833 | Test loss: 0.7880 | Test acc: 0.2500\n",
      "Total training time: 0.026 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 640 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6805 | Train acc: 0.5833 | Test loss: 0.7843 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6793 | Train acc: 0.5833 | Test loss: 0.7806 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.7, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 697 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6774 | Train acc: 0.5833 | Test loss: 0.7999 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6756 | Train acc: 0.5833 | Test loss: 0.7963 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.1\n",
      "System generation finished in 0 minutes 1 seconds 511 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6786 | Train acc: 0.5833 | Test loss: 0.7751 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6770 | Train acc: 0.5833 | Test loss: 0.7774 | Test acc: 0.2500\n",
      "Total training time: 0.024 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.45\n",
      "System generation finished in 0 minutes 1 seconds 627 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6706 | Train acc: 0.5833 | Test loss: 0.7881 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6695 | Train acc: 0.5833 | Test loss: 0.7878 | Test acc: 0.2500\n",
      "Total training time: 0.023 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.7\n",
      "System generation finished in 0 minutes 1 seconds 509 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6792 | Train acc: 0.5833 | Test loss: 0.7989 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6777 | Train acc: 0.5833 | Test loss: 0.7968 | Test acc: 0.2500\n",
      "Total training time: 0.025 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "variance a:0.87, variance b:0.87\n",
      "System generation finished in 0 minutes 1 seconds 589 milliseconds\n",
      "Epoch: 1 | Train loss: 0.6735 | Train acc: 0.5833 | Test loss: 0.7854 | Test acc: 0.2500\n",
      "Epoch: 2 | Train loss: 0.6723 | Train acc: 0.5833 | Test loss: 0.7879 | Test acc: 0.2500\n",
      "Total training time: 0.038 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = b = [0.1, 0.45, 0.7, 0.87]\n",
    "mean = [1, 1]  # Mean for a/c and b/c ratios\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(b)):\n",
    "        variance_a = a[i]  # Variance for a/c\n",
    "        variance_b = b[j]  # Variance for b/c\n",
    "        # Covariance matrix for the bivariate normal distribution\n",
    "        cov = [[variance_a, cov_ab], \n",
    "               [cov_ab, variance_b]]\n",
    "        start = time.time()\n",
    "        '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "        \n",
    "        sphere_img2 = system_maker(2,30,1,200,2,(64,64))\n",
    "        ellips_img2 = system_maker2(2,30,1,200,2,(64,64))\n",
    "        end = time.time()\n",
    "        \n",
    "        #images_bw = np.concatenate((sphere_img, ellips_img), axis=0)\n",
    "        images_cont = np.concatenate((sphere_img2, ellips_img2), axis=0)\n",
    "        \n",
    "        # record time\n",
    "        creation_time = end - start\n",
    "        \n",
    "        label = label_making(0, sphere_img)\n",
    "        label2 = label_making(1, ellips_img)\n",
    "        label.extend(label2)\n",
    "        labels_array = np.array(label)\n",
    "        \n",
    "        #np.savez(f'images_{i}{j}',images_cont=images_cont, images_bw=images_bw, labels=labels_array)\n",
    "        plot_images(images_cont[4],images_cont[0],images_cont[14],images_cont[11],f'image_a{i}_b{j}')\n",
    "        print(f'variance a:{variance_a}, variance b:{variance_b}')\n",
    "        print(f'System generation finished in {creation_time // 60:.0f} minutes {creation_time % 60:.0f} seconds {creation_time % 1 * 1000:.0f} milliseconds')\n",
    "\n",
    "        #Data transformation\n",
    "        data_array = np.array(images_cont)\n",
    "    \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(data_array, labels_array, test_size=0.2, random_state=42)\n",
    "        #print(f\"Training data shape: {X_train.shape}\")\n",
    "        #print(f\"Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        # Instantiate the dataset\n",
    "        BATCH_SIZE = 32\n",
    "        NUM_WORKERS = 0 #os.cpu_count()\n",
    "        \n",
    "        train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "        \n",
    "        #print(f'Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...')\n",
    "\n",
    "        # Trainig begins\n",
    "        # Set no of epochs (newnet)\n",
    "        NUM_EPOCHS = 5\n",
    "        \n",
    "        # Setup loss function and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # Train model\n",
    "        model_results = train(model=model,\n",
    "                             train_data=train_dataloader,\n",
    "                             test_data=val_dataloader,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_fn=loss_fn,\n",
    "                             epochs=NUM_EPOCHS)\n",
    "        # End timer and print out time taken\n",
    "        end_time = timer()\n",
    "        print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "        \n",
    "        # Calculate parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f'Total parameters: {total_params}\\n')\n",
    "        plot_loss_curves(model_results, f'Loss_a{i}_b{j}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0201a9-4677-46fe-8392-a26b54ddafc7",
   "metadata": {},
   "source": [
    "# Qsub script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa50b3-5a58-44cc-bc25-cb6da9c44df7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## not used function\n",
    "# Generate uniform points on surface of sphere for normal projections\n",
    "def projection_points(radius, num_points):\n",
    "    \n",
    "    phi = np.linspace(0, np.pi, num_points)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_points)\n",
    "    theta, phi = np.meshgrid(theta, phi)\n",
    "    \n",
    "    x = radius * np.sin(phi) * np.cos(theta)\n",
    "    y = radius * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * np.cos(phi)\n",
    "    \n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7bb9b5-fb08-4f1c-bbde-927e67758fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.06 GB\n",
      "Used GPU Memory: 0.02 GB\n",
      "Model being used is SimpleCNN\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 15.884 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7017 | Train acc: 0.5025 | Test loss: 0.6624 | Test acc: 0.6458\n",
      "Epoch: 2 | Train loss: 0.6684 | Train acc: 0.5550 | Test loss: 0.6329 | Test acc: 0.6354\n",
      "Epoch: 3 | Train loss: 0.6282 | Train acc: 0.6475 | Test loss: 0.5956 | Test acc: 0.7083\n",
      "Epoch: 4 | Train loss: 0.5895 | Train acc: 0.7175 | Test loss: 0.5416 | Test acc: 0.7917\n",
      "Epoch: 5 | Train loss: 0.5545 | Train acc: 0.7662 | Test loss: 0.4974 | Test acc: 0.8542\n",
      "Epoch: 6 | Train loss: 0.5259 | Train acc: 0.7625 | Test loss: 0.4524 | Test acc: 0.8542\n",
      "Epoch: 7 | Train loss: 0.5008 | Train acc: 0.7750 | Test loss: 0.4317 | Test acc: 0.8542\n",
      "Epoch: 8 | Train loss: 0.4651 | Train acc: 0.8063 | Test loss: 0.3950 | Test acc: 0.8854\n",
      "Epoch: 9 | Train loss: 0.4597 | Train acc: 0.8125 | Test loss: 0.3753 | Test acc: 0.8854\n",
      "Epoch: 10 | Train loss: 0.4362 | Train acc: 0.8137 | Test loss: 0.3521 | Test acc: 0.8958\n",
      "Total training time: 0.894 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 15.555 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7972 | Train acc: 0.5225 | Test loss: 0.7513 | Test acc: 0.5000\n",
      "Epoch: 2 | Train loss: 0.6892 | Train acc: 0.5550 | Test loss: 0.7294 | Test acc: 0.5104\n",
      "Epoch: 3 | Train loss: 0.6822 | Train acc: 0.5563 | Test loss: 0.7330 | Test acc: 0.5000\n",
      "Epoch: 4 | Train loss: 0.6726 | Train acc: 0.5775 | Test loss: 0.7226 | Test acc: 0.5000\n",
      "Epoch: 5 | Train loss: 0.6717 | Train acc: 0.5787 | Test loss: 0.6998 | Test acc: 0.4375\n",
      "Epoch: 6 | Train loss: 0.6627 | Train acc: 0.6138 | Test loss: 0.6969 | Test acc: 0.5312\n",
      "Epoch: 7 | Train loss: 0.6614 | Train acc: 0.5950 | Test loss: 0.7012 | Test acc: 0.5000\n",
      "Epoch: 8 | Train loss: 0.6539 | Train acc: 0.6088 | Test loss: 0.7128 | Test acc: 0.4792\n",
      "Epoch: 9 | Train loss: 0.6515 | Train acc: 0.6300 | Test loss: 0.6935 | Test acc: 0.4792\n",
      "Epoch: 10 | Train loss: 0.6519 | Train acc: 0.6050 | Test loss: 0.6924 | Test acc: 0.5000\n",
      "Total training time: 0.884 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 17.068 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6739 | Train acc: 0.5725 | Test loss: 0.6882 | Test acc: 0.5833\n",
      "Epoch: 2 | Train loss: 0.6651 | Train acc: 0.5887 | Test loss: 0.6788 | Test acc: 0.6250\n",
      "Epoch: 3 | Train loss: 0.6605 | Train acc: 0.5950 | Test loss: 0.6758 | Test acc: 0.5208\n",
      "Epoch: 4 | Train loss: 0.6596 | Train acc: 0.6075 | Test loss: 0.6719 | Test acc: 0.5833\n",
      "Epoch: 5 | Train loss: 0.6547 | Train acc: 0.6300 | Test loss: 0.6710 | Test acc: 0.5625\n",
      "Epoch: 6 | Train loss: 0.6497 | Train acc: 0.6312 | Test loss: 0.6695 | Test acc: 0.5938\n",
      "Epoch: 7 | Train loss: 0.6474 | Train acc: 0.6225 | Test loss: 0.6684 | Test acc: 0.5938\n",
      "Epoch: 8 | Train loss: 0.6411 | Train acc: 0.6663 | Test loss: 0.6757 | Test acc: 0.5104\n",
      "Epoch: 9 | Train loss: 0.6484 | Train acc: 0.6275 | Test loss: 0.6647 | Test acc: 0.6042\n",
      "Epoch: 10 | Train loss: 0.6389 | Train acc: 0.6562 | Test loss: 0.6706 | Test acc: 0.6042\n",
      "Total training time: 0.973 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 15.920 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6718 | Train acc: 0.5563 | Test loss: 0.6795 | Test acc: 0.6042\n",
      "Epoch: 2 | Train loss: 0.6626 | Train acc: 0.6150 | Test loss: 0.6774 | Test acc: 0.5938\n",
      "Epoch: 3 | Train loss: 0.6608 | Train acc: 0.6075 | Test loss: 0.6775 | Test acc: 0.5521\n",
      "Epoch: 4 | Train loss: 0.6558 | Train acc: 0.6275 | Test loss: 0.6784 | Test acc: 0.6250\n",
      "Epoch: 5 | Train loss: 0.6562 | Train acc: 0.6162 | Test loss: 0.6767 | Test acc: 0.5729\n",
      "Epoch: 6 | Train loss: 0.6491 | Train acc: 0.6362 | Test loss: 0.6774 | Test acc: 0.6042\n",
      "Epoch: 7 | Train loss: 0.6489 | Train acc: 0.6625 | Test loss: 0.6776 | Test acc: 0.6354\n",
      "Epoch: 8 | Train loss: 0.6457 | Train acc: 0.6425 | Test loss: 0.6766 | Test acc: 0.5521\n",
      "Epoch: 9 | Train loss: 0.6435 | Train acc: 0.6637 | Test loss: 0.6797 | Test acc: 0.6146\n",
      "Epoch: 10 | Train loss: 0.6397 | Train acc: 0.6700 | Test loss: 0.6775 | Test acc: 0.5417\n",
      "Total training time: 1.028 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 16.493 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7000 | Train acc: 0.5075 | Test loss: 0.7051 | Test acc: 0.4479\n",
      "Epoch: 2 | Train loss: 0.6955 | Train acc: 0.5325 | Test loss: 0.7041 | Test acc: 0.4896\n",
      "Epoch: 3 | Train loss: 0.6909 | Train acc: 0.5312 | Test loss: 0.7017 | Test acc: 0.4792\n",
      "Epoch: 4 | Train loss: 0.6872 | Train acc: 0.5537 | Test loss: 0.7011 | Test acc: 0.4792\n",
      "Epoch: 5 | Train loss: 0.6859 | Train acc: 0.5450 | Test loss: 0.7003 | Test acc: 0.4375\n",
      "Epoch: 6 | Train loss: 0.6832 | Train acc: 0.5450 | Test loss: 0.6994 | Test acc: 0.4583\n",
      "Epoch: 7 | Train loss: 0.6807 | Train acc: 0.5700 | Test loss: 0.6992 | Test acc: 0.4583\n",
      "Epoch: 8 | Train loss: 0.6785 | Train acc: 0.5563 | Test loss: 0.6988 | Test acc: 0.4583\n",
      "Epoch: 9 | Train loss: 0.6785 | Train acc: 0.5613 | Test loss: 0.6976 | Test acc: 0.4583\n",
      "Epoch: 10 | Train loss: 0.6740 | Train acc: 0.5775 | Test loss: 0.6976 | Test acc: 0.4479\n",
      "Total training time: 0.841 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 16.405 seconds\n",
      "Training data shape: (800, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7011 | Train acc: 0.4938 | Test loss: 0.6939 | Test acc: 0.5000\n",
      "Epoch: 2 | Train loss: 0.6984 | Train acc: 0.5012 | Test loss: 0.6938 | Test acc: 0.4896\n",
      "Epoch: 3 | Train loss: 0.6960 | Train acc: 0.5188 | Test loss: 0.6936 | Test acc: 0.4792\n",
      "Epoch: 4 | Train loss: 0.6939 | Train acc: 0.5038 | Test loss: 0.6934 | Test acc: 0.4896\n",
      "Epoch: 5 | Train loss: 0.6915 | Train acc: 0.5238 | Test loss: 0.6931 | Test acc: 0.4792\n",
      "Epoch: 6 | Train loss: 0.6874 | Train acc: 0.5212 | Test loss: 0.6932 | Test acc: 0.4688\n",
      "Epoch: 7 | Train loss: 0.6848 | Train acc: 0.5500 | Test loss: 0.6931 | Test acc: 0.4688\n",
      "Epoch: 8 | Train loss: 0.6850 | Train acc: 0.5350 | Test loss: 0.6928 | Test acc: 0.4479\n",
      "Epoch: 9 | Train loss: 0.6818 | Train acc: 0.5463 | Test loss: 0.6925 | Test acc: 0.4792\n",
      "Epoch: 10 | Train loss: 0.6772 | Train acc: 0.5650 | Test loss: 0.6926 | Test acc: 0.4375\n",
      "Total training time: 0.932 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Change Varience, same mean at 1,1, not log normal\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import platform\n",
    "from scipy.stats import multivariate_normal, lognorm\n",
    "from sklearn.utils import shuffle\n",
    "import os \n",
    "\n",
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\t\n",
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=2).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, log_normal=False, plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, log_normal)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = sphere(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "    \n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "        \n",
    "# CNN model    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res, distance):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1,distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res, distance, status=False):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, log_normal=status)\n",
    "            #a = ellipsoid(max_ellipsoid_size, no_of_points) # create ellipsoid\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1, distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\t\n",
    "\n",
    "## Test script\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "variance_a = 0.3  # Variance for a/c\n",
    "variance_b = 0.3  # Variance for b/c\n",
    "mean = [0, 0]\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "distances = [3, 50, 75, 100, 200, 300]\n",
    "image_name = 'dens_img'\n",
    "loss_name = 'dens_loss'\n",
    "for d in distances:\n",
    "    start = timer()\n",
    "    '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "    \n",
    "        # Generate training images\n",
    "    sphere_img_train = system_maker(200, 30, 1, 100, 2, (64, 64),d)\n",
    "    ellips_img_train = system_maker2(200, 30, 1, 100, 2, (64, 64),d, True)\n",
    "    \n",
    "    # Generate testing images with different parameters\n",
    "    sphere_img_test = system_maker(20, 30, 1, 100, 2, (64, 64),d)\n",
    "    ellips_img_test = system_maker2(20, 30, 1, 100, 2,(64, 64),d, True)\n",
    "\n",
    "    end = timer()\n",
    "    \n",
    "    print(f'\\nvariance a:{variance_a}, variance b:{variance_b}, mean: {mean}, covarient: {cov_ab}')\n",
    "    print(f\"Total generation time: {end-start:.3f} seconds\")\n",
    "    \n",
    "    # Concatenate the training images and labels\n",
    "    images_train = np.concatenate((sphere_img_train, ellips_img_train), axis=0)\n",
    "    labels_train = label_making(0, sphere_img_train) + label_making(1, ellips_img_train)\n",
    "    # plot_images(images_train[4],images_train[0],images_train[14],images_train[11],f'{image_name}_a{i}_b{j}')\n",
    "    plot_images(images_train[4],images_train[0],images_train[14],images_train[11],f'{image_name}_dens{d}')\n",
    "    \n",
    "    # Concatenate the testing images and labels\n",
    "    images_test = np.concatenate((sphere_img_test, ellips_img_test), axis=0)\n",
    "    labels_test = label_making(0, sphere_img_test) + label_making(1, ellips_img_test)\n",
    "    \n",
    "    # Convert labels to numpy arrays\n",
    "    labels_array_train = np.array(labels_train)\n",
    "    labels_array_test = np.array(labels_test)\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    X_train, y_train = shuffle(images_train, labels_array_train, random_state=42)\n",
    "    X_val, y_val = shuffle(images_test, labels_array_test, random_state=42)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\\n\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    # Instantiate the dataset\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 0 #os.cpu_count()\n",
    "    \n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Trainig begins\n",
    "    # Set no of epochs (newnet)\n",
    "    NUM_EPOCHS = 10\n",
    "\n",
    "    # Setup loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Train model\n",
    "    model_results = train(model=model,\n",
    "                         train_data=train_dataloader,\n",
    "                         test_data=val_dataloader,\n",
    "                         optimizer=optimizer,\n",
    "                         loss_fn=loss_fn,\n",
    "                         epochs=NUM_EPOCHS)\n",
    "    # End timer and print out time taken\n",
    "    end_time = timer()\n",
    "    print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "    \n",
    "    # Calculate parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total parameters: {total_params}\\n')\n",
    "    plot_loss_curves(model_results, f'Loss_dens_{d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fed498-98b7-4dee-a522-b12573bbf1cf",
   "metadata": {},
   "source": [
    "# Varience test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a76a0fd-0afb-49f8-8a7e-ca27a8ece0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.06 GB\n",
      "Used GPU Memory: 0.02 GB\n",
      "Model being used is SimpleCNN\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.875 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7006 | Train acc: 0.5288 | Test loss: 0.7192 | Test acc: 0.5000\n",
      "Epoch: 2 | Train loss: 0.7059 | Train acc: 0.4856 | Test loss: 0.7032 | Test acc: 0.5000\n",
      "Epoch: 3 | Train loss: 0.6907 | Train acc: 0.5264 | Test loss: 0.6921 | Test acc: 0.5208\n",
      "Epoch: 4 | Train loss: 0.6833 | Train acc: 0.5721 | Test loss: 0.7178 | Test acc: 0.5000\n",
      "Epoch: 5 | Train loss: 0.7239 | Train acc: 0.4880 | Test loss: 0.6926 | Test acc: 0.5000\n",
      "Epoch: 6 | Train loss: 0.7229 | Train acc: 0.5312 | Test loss: 0.7073 | Test acc: 0.5000\n",
      "Epoch: 7 | Train loss: 0.7083 | Train acc: 0.4976 | Test loss: 0.6886 | Test acc: 0.5208\n",
      "Epoch: 8 | Train loss: 0.7028 | Train acc: 0.5144 | Test loss: 0.7009 | Test acc: 0.5000\n",
      "Epoch: 9 | Train loss: 0.6983 | Train acc: 0.5481 | Test loss: 0.7230 | Test acc: 0.5000\n",
      "Epoch: 10 | Train loss: 0.6871 | Train acc: 0.5505 | Test loss: 0.6865 | Test acc: 0.4792\n",
      "Total training time: 0.769 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 8.916 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6905 | Train acc: 0.5312 | Test loss: 0.6836 | Test acc: 0.5104\n",
      "Epoch: 2 | Train loss: 0.7210 | Train acc: 0.4639 | Test loss: 0.7025 | Test acc: 0.5000\n",
      "Epoch: 3 | Train loss: 0.6929 | Train acc: 0.5264 | Test loss: 0.7044 | Test acc: 0.5000\n",
      "Epoch: 4 | Train loss: 0.6813 | Train acc: 0.5601 | Test loss: 0.6822 | Test acc: 0.6042\n",
      "Epoch: 5 | Train loss: 0.6719 | Train acc: 0.5673 | Test loss: 0.6879 | Test acc: 0.5521\n",
      "Epoch: 6 | Train loss: 0.6824 | Train acc: 0.5553 | Test loss: 0.6924 | Test acc: 0.5104\n",
      "Epoch: 7 | Train loss: 0.6734 | Train acc: 0.5529 | Test loss: 0.6907 | Test acc: 0.5208\n",
      "Epoch: 8 | Train loss: 0.6759 | Train acc: 0.5793 | Test loss: 0.6903 | Test acc: 0.5000\n",
      "Epoch: 9 | Train loss: 0.6586 | Train acc: 0.6130 | Test loss: 0.6929 | Test acc: 0.5000\n",
      "Epoch: 10 | Train loss: 0.6526 | Train acc: 0.5745 | Test loss: 0.6900 | Test acc: 0.5208\n",
      "Total training time: 0.662 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.369 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7048 | Train acc: 0.5216 | Test loss: 0.6822 | Test acc: 0.5938\n",
      "Epoch: 2 | Train loss: 0.7041 | Train acc: 0.5144 | Test loss: 0.6966 | Test acc: 0.5000\n",
      "Epoch: 3 | Train loss: 0.6768 | Train acc: 0.5721 | Test loss: 0.6890 | Test acc: 0.5000\n",
      "Epoch: 4 | Train loss: 0.6674 | Train acc: 0.6034 | Test loss: 0.6760 | Test acc: 0.5938\n",
      "Epoch: 5 | Train loss: 0.6629 | Train acc: 0.5889 | Test loss: 0.6789 | Test acc: 0.4688\n",
      "Epoch: 6 | Train loss: 0.6645 | Train acc: 0.5841 | Test loss: 0.6712 | Test acc: 0.6146\n",
      "Epoch: 7 | Train loss: 0.6641 | Train acc: 0.5721 | Test loss: 0.6745 | Test acc: 0.5625\n",
      "Epoch: 8 | Train loss: 0.6568 | Train acc: 0.5986 | Test loss: 0.6762 | Test acc: 0.5729\n",
      "Epoch: 9 | Train loss: 0.6450 | Train acc: 0.6514 | Test loss: 0.6652 | Test acc: 0.6458\n",
      "Epoch: 10 | Train loss: 0.6388 | Train acc: 0.7067 | Test loss: 0.6627 | Test acc: 0.6562\n",
      "Total training time: 0.566 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 8.972 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6658 | Train acc: 0.6442 | Test loss: 0.6380 | Test acc: 0.7292\n",
      "Epoch: 2 | Train loss: 0.6660 | Train acc: 0.5962 | Test loss: 0.6378 | Test acc: 0.6771\n",
      "Epoch: 3 | Train loss: 0.6695 | Train acc: 0.5529 | Test loss: 0.6725 | Test acc: 0.5000\n",
      "Epoch: 4 | Train loss: 0.6738 | Train acc: 0.5769 | Test loss: 0.6341 | Test acc: 0.6875\n",
      "Epoch: 5 | Train loss: 0.6586 | Train acc: 0.5817 | Test loss: 0.6316 | Test acc: 0.6875\n",
      "Epoch: 6 | Train loss: 0.6623 | Train acc: 0.5962 | Test loss: 0.6648 | Test acc: 0.5208\n",
      "Epoch: 7 | Train loss: 0.6543 | Train acc: 0.5986 | Test loss: 0.6386 | Test acc: 0.5729\n",
      "Epoch: 8 | Train loss: 0.6471 | Train acc: 0.6418 | Test loss: 0.6202 | Test acc: 0.6875\n",
      "Epoch: 9 | Train loss: 0.6266 | Train acc: 0.6731 | Test loss: 0.6219 | Test acc: 0.6771\n",
      "Epoch: 10 | Train loss: 0.6332 | Train acc: 0.6514 | Test loss: 0.6149 | Test acc: 0.7396\n",
      "Total training time: 0.736 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.345 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6638 | Train acc: 0.6010 | Test loss: 0.6562 | Test acc: 0.6042\n",
      "Epoch: 2 | Train loss: 0.6555 | Train acc: 0.6226 | Test loss: 0.6549 | Test acc: 0.6146\n",
      "Epoch: 3 | Train loss: 0.6523 | Train acc: 0.6370 | Test loss: 0.6549 | Test acc: 0.6146\n",
      "Epoch: 4 | Train loss: 0.6460 | Train acc: 0.6082 | Test loss: 0.6593 | Test acc: 0.5938\n",
      "Epoch: 5 | Train loss: 0.6473 | Train acc: 0.6346 | Test loss: 0.6575 | Test acc: 0.5938\n",
      "Epoch: 6 | Train loss: 0.6453 | Train acc: 0.6394 | Test loss: 0.6514 | Test acc: 0.6250\n",
      "Epoch: 7 | Train loss: 0.6266 | Train acc: 0.6635 | Test loss: 0.6444 | Test acc: 0.6354\n",
      "Epoch: 8 | Train loss: 0.6218 | Train acc: 0.6779 | Test loss: 0.6707 | Test acc: 0.5833\n",
      "Epoch: 9 | Train loss: 0.6225 | Train acc: 0.6611 | Test loss: 0.6413 | Test acc: 0.6354\n",
      "Epoch: 10 | Train loss: 0.6074 | Train acc: 0.7043 | Test loss: 0.6416 | Test acc: 0.6771\n",
      "Total training time: 0.644 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.382 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5964 | Train acc: 0.7428 | Test loss: 0.6290 | Test acc: 0.6458\n",
      "Epoch: 2 | Train loss: 0.6098 | Train acc: 0.6755 | Test loss: 0.6279 | Test acc: 0.6354\n",
      "Epoch: 3 | Train loss: 0.5984 | Train acc: 0.7019 | Test loss: 0.6284 | Test acc: 0.6562\n",
      "Epoch: 4 | Train loss: 0.5933 | Train acc: 0.7067 | Test loss: 0.6181 | Test acc: 0.6562\n",
      "Epoch: 5 | Train loss: 0.5672 | Train acc: 0.7236 | Test loss: 0.6352 | Test acc: 0.6146\n",
      "Epoch: 6 | Train loss: 0.5682 | Train acc: 0.7692 | Test loss: 0.6131 | Test acc: 0.6771\n",
      "Epoch: 7 | Train loss: 0.5522 | Train acc: 0.7668 | Test loss: 0.6197 | Test acc: 0.6250\n",
      "Epoch: 8 | Train loss: 0.5561 | Train acc: 0.7428 | Test loss: 0.6189 | Test acc: 0.6667\n",
      "Epoch: 9 | Train loss: 0.5543 | Train acc: 0.7188 | Test loss: 0.6187 | Test acc: 0.5938\n",
      "Epoch: 10 | Train loss: 0.5383 | Train acc: 0.7620 | Test loss: 0.6033 | Test acc: 0.6562\n",
      "Total training time: 0.635 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.623 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5952 | Train acc: 0.6635 | Test loss: 0.5662 | Test acc: 0.7812\n",
      "Epoch: 2 | Train loss: 0.5762 | Train acc: 0.7356 | Test loss: 0.5637 | Test acc: 0.8021\n",
      "Epoch: 3 | Train loss: 0.5775 | Train acc: 0.7115 | Test loss: 0.5618 | Test acc: 0.7917\n",
      "Epoch: 4 | Train loss: 0.5736 | Train acc: 0.7163 | Test loss: 0.5600 | Test acc: 0.7812\n",
      "Epoch: 5 | Train loss: 0.5499 | Train acc: 0.7764 | Test loss: 0.5587 | Test acc: 0.7812\n",
      "Epoch: 6 | Train loss: 0.5574 | Train acc: 0.7332 | Test loss: 0.5588 | Test acc: 0.7812\n",
      "Epoch: 7 | Train loss: 0.5428 | Train acc: 0.7620 | Test loss: 0.5554 | Test acc: 0.7604\n",
      "Epoch: 8 | Train loss: 0.5411 | Train acc: 0.7885 | Test loss: 0.5529 | Test acc: 0.7812\n",
      "Epoch: 9 | Train loss: 0.5323 | Train acc: 0.7837 | Test loss: 0.5518 | Test acc: 0.7500\n",
      "Epoch: 10 | Train loss: 0.5099 | Train acc: 0.8221 | Test loss: 0.5541 | Test acc: 0.7500\n",
      "Total training time: 0.552 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.757 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5456 | Train acc: 0.7236 | Test loss: 0.6096 | Test acc: 0.6875\n",
      "Epoch: 2 | Train loss: 0.5378 | Train acc: 0.7692 | Test loss: 0.6128 | Test acc: 0.6875\n",
      "Epoch: 3 | Train loss: 0.5198 | Train acc: 0.7957 | Test loss: 0.6081 | Test acc: 0.6562\n",
      "Epoch: 4 | Train loss: 0.5187 | Train acc: 0.7788 | Test loss: 0.6193 | Test acc: 0.6562\n",
      "Epoch: 5 | Train loss: 0.5013 | Train acc: 0.8077 | Test loss: 0.6075 | Test acc: 0.6979\n",
      "Epoch: 6 | Train loss: 0.4881 | Train acc: 0.8341 | Test loss: 0.6110 | Test acc: 0.6458\n",
      "Epoch: 7 | Train loss: 0.4821 | Train acc: 0.8317 | Test loss: 0.6062 | Test acc: 0.6979\n",
      "Epoch: 8 | Train loss: 0.4840 | Train acc: 0.8149 | Test loss: 0.6118 | Test acc: 0.6667\n",
      "Epoch: 9 | Train loss: 0.4590 | Train acc: 0.8486 | Test loss: 0.6136 | Test acc: 0.6458\n",
      "Epoch: 10 | Train loss: 0.4685 | Train acc: 0.8149 | Test loss: 0.6136 | Test acc: 0.6354\n",
      "Total training time: 0.624 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.096 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5919 | Train acc: 0.6707 | Test loss: 0.5328 | Test acc: 0.7083\n",
      "Epoch: 2 | Train loss: 0.5718 | Train acc: 0.7067 | Test loss: 0.5336 | Test acc: 0.7812\n",
      "Epoch: 3 | Train loss: 0.5705 | Train acc: 0.7043 | Test loss: 0.5218 | Test acc: 0.7500\n",
      "Epoch: 4 | Train loss: 0.5522 | Train acc: 0.7332 | Test loss: 0.5206 | Test acc: 0.7604\n",
      "Epoch: 5 | Train loss: 0.5534 | Train acc: 0.7284 | Test loss: 0.5239 | Test acc: 0.7917\n",
      "Epoch: 6 | Train loss: 0.5679 | Train acc: 0.7188 | Test loss: 0.5317 | Test acc: 0.7812\n",
      "Epoch: 7 | Train loss: 0.5548 | Train acc: 0.7236 | Test loss: 0.5142 | Test acc: 0.7292\n",
      "Epoch: 8 | Train loss: 0.5320 | Train acc: 0.7740 | Test loss: 0.5126 | Test acc: 0.7604\n",
      "Epoch: 9 | Train loss: 0.5338 | Train acc: 0.7404 | Test loss: 0.5104 | Test acc: 0.7812\n",
      "Epoch: 10 | Train loss: 0.5252 | Train acc: 0.7668 | Test loss: 0.5104 | Test acc: 0.7396\n",
      "Total training time: 0.635 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.364 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5419 | Train acc: 0.7212 | Test loss: 0.5126 | Test acc: 0.7604\n",
      "Epoch: 2 | Train loss: 0.5214 | Train acc: 0.7428 | Test loss: 0.5027 | Test acc: 0.7708\n",
      "Epoch: 3 | Train loss: 0.5367 | Train acc: 0.7139 | Test loss: 0.5239 | Test acc: 0.7604\n",
      "Epoch: 4 | Train loss: 0.5444 | Train acc: 0.7332 | Test loss: 0.4995 | Test acc: 0.7708\n",
      "Epoch: 5 | Train loss: 0.5211 | Train acc: 0.7596 | Test loss: 0.5036 | Test acc: 0.7604\n",
      "Epoch: 6 | Train loss: 0.5089 | Train acc: 0.7572 | Test loss: 0.4943 | Test acc: 0.7708\n",
      "Epoch: 7 | Train loss: 0.4938 | Train acc: 0.7861 | Test loss: 0.4941 | Test acc: 0.7708\n",
      "Epoch: 8 | Train loss: 0.4894 | Train acc: 0.7957 | Test loss: 0.4897 | Test acc: 0.8021\n",
      "Epoch: 9 | Train loss: 0.4812 | Train acc: 0.7909 | Test loss: 0.4868 | Test acc: 0.7708\n",
      "Epoch: 10 | Train loss: 0.4788 | Train acc: 0.7957 | Test loss: 0.4891 | Test acc: 0.7812\n",
      "Total training time: 0.634 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 10.021 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4984 | Train acc: 0.7692 | Test loss: 0.4864 | Test acc: 0.7708\n",
      "Epoch: 2 | Train loss: 0.5006 | Train acc: 0.7740 | Test loss: 0.4776 | Test acc: 0.7708\n",
      "Epoch: 3 | Train loss: 0.4718 | Train acc: 0.7981 | Test loss: 0.4767 | Test acc: 0.7708\n",
      "Epoch: 4 | Train loss: 0.4880 | Train acc: 0.7764 | Test loss: 0.4887 | Test acc: 0.7396\n",
      "Epoch: 5 | Train loss: 0.4634 | Train acc: 0.7981 | Test loss: 0.4723 | Test acc: 0.7917\n",
      "Epoch: 6 | Train loss: 0.4585 | Train acc: 0.8125 | Test loss: 0.4665 | Test acc: 0.7917\n",
      "Epoch: 7 | Train loss: 0.4617 | Train acc: 0.8005 | Test loss: 0.4616 | Test acc: 0.7812\n",
      "Epoch: 8 | Train loss: 0.4414 | Train acc: 0.8269 | Test loss: 0.4619 | Test acc: 0.8021\n",
      "Epoch: 9 | Train loss: 0.4450 | Train acc: 0.8029 | Test loss: 0.4592 | Test acc: 0.7917\n",
      "Epoch: 10 | Train loss: 0.4250 | Train acc: 0.8462 | Test loss: 0.4532 | Test acc: 0.7812\n",
      "Total training time: 0.733 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.859 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4790 | Train acc: 0.7861 | Test loss: 0.3953 | Test acc: 0.8750\n",
      "Epoch: 2 | Train loss: 0.4663 | Train acc: 0.7861 | Test loss: 0.3944 | Test acc: 0.8750\n",
      "Epoch: 3 | Train loss: 0.4545 | Train acc: 0.8077 | Test loss: 0.4024 | Test acc: 0.8229\n",
      "Epoch: 4 | Train loss: 0.4558 | Train acc: 0.7837 | Test loss: 0.3928 | Test acc: 0.8750\n",
      "Epoch: 5 | Train loss: 0.4474 | Train acc: 0.8125 | Test loss: 0.3921 | Test acc: 0.8750\n",
      "Epoch: 6 | Train loss: 0.4364 | Train acc: 0.8125 | Test loss: 0.3903 | Test acc: 0.8750\n",
      "Epoch: 7 | Train loss: 0.4284 | Train acc: 0.8173 | Test loss: 0.3905 | Test acc: 0.8542\n",
      "Epoch: 8 | Train loss: 0.4218 | Train acc: 0.8389 | Test loss: 0.3885 | Test acc: 0.8854\n",
      "Epoch: 9 | Train loss: 0.4352 | Train acc: 0.7957 | Test loss: 0.4099 | Test acc: 0.8333\n",
      "Epoch: 10 | Train loss: 0.4191 | Train acc: 0.8389 | Test loss: 0.3869 | Test acc: 0.8854\n",
      "Total training time: 0.592 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.389 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4720 | Train acc: 0.7861 | Test loss: 0.4934 | Test acc: 0.7604\n",
      "Epoch: 2 | Train loss: 0.4633 | Train acc: 0.7981 | Test loss: 0.5117 | Test acc: 0.7292\n",
      "Epoch: 3 | Train loss: 0.4620 | Train acc: 0.7909 | Test loss: 0.4927 | Test acc: 0.7396\n",
      "Epoch: 4 | Train loss: 0.4532 | Train acc: 0.8077 | Test loss: 0.4989 | Test acc: 0.7188\n",
      "Epoch: 5 | Train loss: 0.4599 | Train acc: 0.7885 | Test loss: 0.5013 | Test acc: 0.7292\n",
      "Epoch: 6 | Train loss: 0.4497 | Train acc: 0.8053 | Test loss: 0.4920 | Test acc: 0.7500\n",
      "Epoch: 7 | Train loss: 0.4503 | Train acc: 0.7981 | Test loss: 0.4922 | Test acc: 0.7500\n",
      "Epoch: 8 | Train loss: 0.4316 | Train acc: 0.8365 | Test loss: 0.4907 | Test acc: 0.7396\n",
      "Epoch: 9 | Train loss: 0.4375 | Train acc: 0.8149 | Test loss: 0.4930 | Test acc: 0.7188\n",
      "Epoch: 10 | Train loss: 0.4192 | Train acc: 0.8317 | Test loss: 0.4913 | Test acc: 0.7292\n",
      "Total training time: 0.610 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 10.182 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4514 | Train acc: 0.7861 | Test loss: 0.4742 | Test acc: 0.7500\n",
      "Epoch: 2 | Train loss: 0.4530 | Train acc: 0.7861 | Test loss: 0.4594 | Test acc: 0.7604\n",
      "Epoch: 3 | Train loss: 0.4490 | Train acc: 0.8101 | Test loss: 0.4618 | Test acc: 0.7604\n",
      "Epoch: 4 | Train loss: 0.4505 | Train acc: 0.7909 | Test loss: 0.4588 | Test acc: 0.7604\n",
      "Epoch: 5 | Train loss: 0.4410 | Train acc: 0.8101 | Test loss: 0.4573 | Test acc: 0.7604\n",
      "Epoch: 6 | Train loss: 0.4258 | Train acc: 0.8149 | Test loss: 0.4716 | Test acc: 0.7396\n",
      "Epoch: 7 | Train loss: 0.4374 | Train acc: 0.8101 | Test loss: 0.4628 | Test acc: 0.7396\n",
      "Epoch: 8 | Train loss: 0.4114 | Train acc: 0.8197 | Test loss: 0.4531 | Test acc: 0.7396\n",
      "Epoch: 9 | Train loss: 0.4141 | Train acc: 0.8438 | Test loss: 0.4620 | Test acc: 0.7396\n",
      "Epoch: 10 | Train loss: 0.4166 | Train acc: 0.8269 | Test loss: 0.4525 | Test acc: 0.7396\n",
      "Total training time: 0.609 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 10.395 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4240 | Train acc: 0.8269 | Test loss: 0.5070 | Test acc: 0.7500\n",
      "Epoch: 2 | Train loss: 0.4201 | Train acc: 0.8053 | Test loss: 0.5085 | Test acc: 0.7500\n",
      "Epoch: 3 | Train loss: 0.4096 | Train acc: 0.8005 | Test loss: 0.5031 | Test acc: 0.7708\n",
      "Epoch: 4 | Train loss: 0.4007 | Train acc: 0.8389 | Test loss: 0.5031 | Test acc: 0.7708\n",
      "Epoch: 5 | Train loss: 0.3990 | Train acc: 0.8389 | Test loss: 0.5019 | Test acc: 0.7708\n",
      "Epoch: 6 | Train loss: 0.3899 | Train acc: 0.8245 | Test loss: 0.5014 | Test acc: 0.7396\n",
      "Epoch: 7 | Train loss: 0.3838 | Train acc: 0.8389 | Test loss: 0.4984 | Test acc: 0.7917\n",
      "Epoch: 8 | Train loss: 0.3896 | Train acc: 0.8365 | Test loss: 0.4991 | Test acc: 0.7396\n",
      "Epoch: 9 | Train loss: 0.3818 | Train acc: 0.8413 | Test loss: 0.4970 | Test acc: 0.7917\n",
      "Epoch: 10 | Train loss: 0.3805 | Train acc: 0.8341 | Test loss: 0.4983 | Test acc: 0.7500\n",
      "Total training time: 0.599 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:[0.1, 0.5, 1, 1.2], variance b:[0.1, 0.5, 1, 1.2], mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 9.598 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4441 | Train acc: 0.8029 | Test loss: 0.4216 | Test acc: 0.7917\n",
      "Epoch: 2 | Train loss: 0.4232 | Train acc: 0.8173 | Test loss: 0.4210 | Test acc: 0.7708\n",
      "Epoch: 3 | Train loss: 0.4161 | Train acc: 0.8245 | Test loss: 0.4255 | Test acc: 0.7708\n",
      "Epoch: 4 | Train loss: 0.4123 | Train acc: 0.8101 | Test loss: 0.4167 | Test acc: 0.7917\n",
      "Epoch: 5 | Train loss: 0.4179 | Train acc: 0.8101 | Test loss: 0.4187 | Test acc: 0.7812\n",
      "Epoch: 6 | Train loss: 0.4161 | Train acc: 0.8125 | Test loss: 0.4146 | Test acc: 0.7812\n",
      "Epoch: 7 | Train loss: 0.4035 | Train acc: 0.8317 | Test loss: 0.4195 | Test acc: 0.7812\n",
      "Epoch: 8 | Train loss: 0.3933 | Train acc: 0.8365 | Test loss: 0.4139 | Test acc: 0.8021\n",
      "Epoch: 9 | Train loss: 0.3949 | Train acc: 0.8269 | Test loss: 0.4152 | Test acc: 0.7812\n",
      "Epoch: 10 | Train loss: 0.3903 | Train acc: 0.8582 | Test loss: 0.4122 | Test acc: 0.8021\n",
      "Total training time: 0.728 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Change Varience, same mean at 1,1, not log normal\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import platform\n",
    "from scipy.stats import multivariate_normal, lognorm\n",
    "from sklearn.utils import shuffle\n",
    "import os \n",
    "\n",
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\t\n",
    "def generate_ratios(mean, cov, log_normal=False):\n",
    "    if log_normal:\n",
    "        # Generate log-normal distributed ratios for positive and skewed data\n",
    "        s = np.sqrt(np.diag(cov))\n",
    "        scale = np.exp(mean)\n",
    "        a_c, b_c = lognorm.rvs(s=s, scale=scale, size=2).flatten()\n",
    "    else:\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    return a_c, b_c\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, log_normal=False, plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, log_normal)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = sphere(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "    \n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "        \n",
    "# CNN model    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res, distance):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1,distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res, distance, status=False):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, log_normal=status)\n",
    "            #a = ellipsoid(max_ellipsoid_size, no_of_points) # create ellipsoid\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1, distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\t\n",
    "\n",
    "## Test script\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "variance_a = variance_b= [0.1, 0.5, 1, 1.2]\n",
    "mean = [1, 1]\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "d = 40\n",
    "image_name = 'var_img'\n",
    "loss_name = 'var_loss'\n",
    "\n",
    "for i in range(len(variance_a)):\n",
    "    for j in range(len(variance_b)):\n",
    "        cov = [[variance_a[i], cov_ab], \n",
    "               [cov_ab, variance_b[j]]]\n",
    "        start = timer()\n",
    "        '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "        # Generate training images\n",
    "        sphere_img_train = system_maker(100, 30, 1, 100, 2, (64, 64),d)\n",
    "        ellips_img_train = system_maker2(100, 30, 1, 100, 2, (64, 64),d, False)\n",
    "        \n",
    "        # Generate testing images with different parameters\n",
    "        sphere_img_test = system_maker(20, 30, 1, 100, 2, (64, 64),d)\n",
    "        ellips_img_test = system_maker2(20, 30, 1, 100, 2,(64, 64),d, False)\n",
    "    \n",
    "        end = timer()\n",
    "        \n",
    "        print(f'\\nvariance a:{variance_a}, variance b:{variance_b}, mean: {mean}, covarient: {cov_ab}')\n",
    "        print(f\"Total generation time: {end-start:.3f} seconds\")\n",
    "        \n",
    "        # Concatenate the training images and labels\n",
    "        images_train = np.concatenate((sphere_img_train, ellips_img_train), axis=0)\n",
    "        labels_train = label_making(0, sphere_img_train) + label_making(1, ellips_img_train)\n",
    "        # plot_images(images_train[4],images_train[0],images_train[14],images_train[11],f'{image_name}_a{i}_b{j}')\n",
    "        plot_images(images_train[4],images_train[0],images_train[200],images_train[201],f'{image_name}_dens{d}')\n",
    "        \n",
    "        # Concatenate the testing images and labels\n",
    "        images_test = np.concatenate((sphere_img_test, ellips_img_test), axis=0)\n",
    "        labels_test = label_making(0, sphere_img_test) + label_making(1, ellips_img_test)\n",
    "        \n",
    "        # Convert labels to numpy arrays\n",
    "        labels_array_train = np.array(labels_train)\n",
    "        labels_array_test = np.array(labels_test)\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        X_train, y_train = shuffle(images_train, labels_array_train, random_state=42)\n",
    "        X_val, y_val = shuffle(images_test, labels_array_test, random_state=42)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\\n\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        # Instantiate the dataset\n",
    "        BATCH_SIZE = 32\n",
    "        NUM_WORKERS = 0 #os.cpu_count()\n",
    "        \n",
    "        train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "    \n",
    "        # Trainig begins\n",
    "        # Set no of epochs (newnet)\n",
    "        NUM_EPOCHS = 10\n",
    "    \n",
    "        # Setup loss function and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # Train model\n",
    "        model_results = train(model=model,\n",
    "                             train_data=train_dataloader,\n",
    "                             test_data=val_dataloader,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_fn=loss_fn,\n",
    "                             epochs=NUM_EPOCHS)\n",
    "        # End timer and print out time taken\n",
    "        end_time = timer()\n",
    "        print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "        \n",
    "        # Calculate parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f'Total parameters: {total_params}\\n')\n",
    "        plot_loss_curves(model_results, f'Loss_dens_{d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fd6be-2b7b-4cef-a7b8-a3fd219a539e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Mean test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f448ff0-75fa-421e-88c6-bf1c473bcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "\n",
      "NVIDIA GeForce RTX 2070\n",
      "\n",
      "Total GPU Memory: 8.00 GB\n",
      "Free GPU Memory: 0.06 GB\n",
      "Used GPU Memory: 0.02 GB\n",
      "Model being used is SimpleCNN\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0], covarient: 0.01\n",
      "Total generation time: 8.874 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.6958 | Test acc: 0.5208\n",
      "\n",
      "Epoch: 1 | Train loss: 0.7080 | Train acc: 0.4351 | Test loss: 0.6978 | Test acc: 0.5000\n",
      "Epoch: 2 | Train loss: 0.6961 | Train acc: 0.5072 | Test loss: 0.6824 | Test acc: 0.6771\n",
      "Epoch: 3 | Train loss: 0.6775 | Train acc: 0.5505 | Test loss: 0.7059 | Test acc: 0.5000\n",
      "Epoch: 4 | Train loss: 0.6860 | Train acc: 0.5505 | Test loss: 0.6962 | Test acc: 0.5000\n",
      "Epoch: 5 | Train loss: 0.6599 | Train acc: 0.5601 | Test loss: 0.6596 | Test acc: 0.6875\n",
      "Epoch: 6 | Train loss: 0.6509 | Train acc: 0.6538 | Test loss: 0.6504 | Test acc: 0.8333\n",
      "Epoch: 7 | Train loss: 0.6439 | Train acc: 0.6298 | Test loss: 0.6594 | Test acc: 0.5104\n",
      "Epoch: 8 | Train loss: 0.6150 | Train acc: 0.7356 | Test loss: 0.6467 | Test acc: 0.5000\n",
      "Epoch: 9 | Train loss: 0.6065 | Train acc: 0.7284 | Test loss: 0.6247 | Test acc: 0.7604\n",
      "Epoch: 10 | Train loss: 0.6138 | Train acc: 0.6707 | Test loss: 0.6158 | Test acc: 0.7604\n",
      "Total training time: 0.559 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 0.5], covarient: 0.01\n",
      "Total generation time: 8.983 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.6157 | Test acc: 0.8125\n",
      "\n",
      "Epoch: 1 | Train loss: 0.6538 | Train acc: 0.6250 | Test loss: 0.7059 | Test acc: 0.5000\n",
      "Epoch: 2 | Train loss: 0.6438 | Train acc: 0.6178 | Test loss: 0.7119 | Test acc: 0.5000\n",
      "Epoch: 3 | Train loss: 0.6456 | Train acc: 0.5986 | Test loss: 0.6005 | Test acc: 0.8750\n",
      "Epoch: 4 | Train loss: 0.6021 | Train acc: 0.7356 | Test loss: 0.6051 | Test acc: 0.6354\n",
      "Epoch: 5 | Train loss: 0.5985 | Train acc: 0.7212 | Test loss: 0.5887 | Test acc: 0.8021\n",
      "Epoch: 6 | Train loss: 0.5987 | Train acc: 0.6562 | Test loss: 0.5977 | Test acc: 0.5625\n",
      "Epoch: 7 | Train loss: 0.5773 | Train acc: 0.6755 | Test loss: 0.5739 | Test acc: 0.8021\n",
      "Epoch: 8 | Train loss: 0.5475 | Train acc: 0.7981 | Test loss: 0.5632 | Test acc: 0.8229\n",
      "Epoch: 9 | Train loss: 0.5562 | Train acc: 0.7957 | Test loss: 0.5506 | Test acc: 0.8646\n",
      "Epoch: 10 | Train loss: 0.5316 | Train acc: 0.8365 | Test loss: 0.5413 | Test acc: 0.8542\n",
      "Total training time: 0.658 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 1], covarient: 0.01\n",
      "Total generation time: 9.062 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.5379 | Test acc: 0.9271\n",
      "\n",
      "Epoch: 1 | Train loss: 0.5621 | Train acc: 0.7572 | Test loss: 0.5335 | Test acc: 0.9271\n",
      "Epoch: 2 | Train loss: 0.5370 | Train acc: 0.8462 | Test loss: 0.5398 | Test acc: 0.7812\n",
      "Epoch: 3 | Train loss: 0.5466 | Train acc: 0.7644 | Test loss: 0.5322 | Test acc: 0.7812\n",
      "Epoch: 4 | Train loss: 0.5324 | Train acc: 0.7500 | Test loss: 0.5587 | Test acc: 0.5833\n",
      "Epoch: 5 | Train loss: 0.5282 | Train acc: 0.7524 | Test loss: 0.5145 | Test acc: 0.7812\n",
      "Epoch: 6 | Train loss: 0.5092 | Train acc: 0.7909 | Test loss: 0.4956 | Test acc: 0.8438\n",
      "Epoch: 7 | Train loss: 0.5057 | Train acc: 0.7981 | Test loss: 0.4814 | Test acc: 0.9062\n",
      "Epoch: 8 | Train loss: 0.4735 | Train acc: 0.8101 | Test loss: 0.4745 | Test acc: 0.8958\n",
      "Epoch: 9 | Train loss: 0.4740 | Train acc: 0.8341 | Test loss: 0.4628 | Test acc: 0.9375\n",
      "Epoch: 10 | Train loss: 0.4671 | Train acc: 0.8389 | Test loss: 0.4592 | Test acc: 0.8333\n",
      "Total training time: 0.538 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 1.5], covarient: 0.01\n",
      "Total generation time: 8.558 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.5002 | Test acc: 0.8750\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4713 | Train acc: 0.8293 | Test loss: 0.5016 | Test acc: 0.8333\n",
      "Epoch: 2 | Train loss: 0.4963 | Train acc: 0.7861 | Test loss: 0.5357 | Test acc: 0.7083\n",
      "Epoch: 3 | Train loss: 0.4979 | Train acc: 0.7861 | Test loss: 0.5056 | Test acc: 0.7604\n",
      "Epoch: 4 | Train loss: 0.4437 | Train acc: 0.8365 | Test loss: 0.5048 | Test acc: 0.7604\n",
      "Epoch: 5 | Train loss: 0.4533 | Train acc: 0.8245 | Test loss: 0.4670 | Test acc: 0.8750\n",
      "Epoch: 6 | Train loss: 0.4470 | Train acc: 0.8053 | Test loss: 0.4663 | Test acc: 0.8438\n",
      "Epoch: 7 | Train loss: 0.4353 | Train acc: 0.8077 | Test loss: 0.4927 | Test acc: 0.7500\n",
      "Epoch: 8 | Train loss: 0.4295 | Train acc: 0.8173 | Test loss: 0.4844 | Test acc: 0.7500\n",
      "Epoch: 9 | Train loss: 0.4202 | Train acc: 0.8534 | Test loss: 0.4536 | Test acc: 0.8333\n",
      "Epoch: 10 | Train loss: 0.4115 | Train acc: 0.8389 | Test loss: 0.4408 | Test acc: 0.8542\n",
      "Total training time: 0.564 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0, 2], covarient: 0.01\n",
      "Total generation time: 8.921 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.4233 | Test acc: 0.8229\n",
      "\n",
      "Epoch: 1 | Train loss: 0.4335 | Train acc: 0.8245 | Test loss: 0.4153 | Test acc: 0.8438\n",
      "Epoch: 2 | Train loss: 0.4109 | Train acc: 0.8558 | Test loss: 0.4088 | Test acc: 0.8854\n",
      "Epoch: 3 | Train loss: 0.4056 | Train acc: 0.8534 | Test loss: 0.4253 | Test acc: 0.8125\n",
      "Epoch: 4 | Train loss: 0.4388 | Train acc: 0.8005 | Test loss: 0.4514 | Test acc: 0.7917\n",
      "Epoch: 5 | Train loss: 0.4057 | Train acc: 0.8149 | Test loss: 0.4788 | Test acc: 0.7396\n",
      "Epoch: 6 | Train loss: 0.4154 | Train acc: 0.8005 | Test loss: 0.3892 | Test acc: 0.8646\n",
      "Epoch: 7 | Train loss: 0.3878 | Train acc: 0.8486 | Test loss: 0.4351 | Test acc: 0.8125\n",
      "Epoch: 8 | Train loss: 0.3811 | Train acc: 0.8558 | Test loss: 0.4186 | Test acc: 0.8021\n",
      "Epoch: 9 | Train loss: 0.3677 | Train acc: 0.8558 | Test loss: 0.3761 | Test acc: 0.8854\n",
      "Epoch: 10 | Train loss: 0.3408 | Train acc: 0.8894 | Test loss: 0.3811 | Test acc: 0.8438\n",
      "Total training time: 0.624 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0.5, 0], covarient: 0.01\n",
      "Total generation time: 8.541 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3154 | Test acc: 0.8854\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3665 | Train acc: 0.8654 | Test loss: 0.3015 | Test acc: 0.9271\n",
      "Epoch: 2 | Train loss: 0.3395 | Train acc: 0.8822 | Test loss: 0.2944 | Test acc: 0.9062\n",
      "Epoch: 3 | Train loss: 0.3207 | Train acc: 0.8990 | Test loss: 0.2890 | Test acc: 0.9167\n",
      "Epoch: 4 | Train loss: 0.3159 | Train acc: 0.8918 | Test loss: 0.2935 | Test acc: 0.8854\n",
      "Epoch: 5 | Train loss: 0.3085 | Train acc: 0.9087 | Test loss: 0.2874 | Test acc: 0.9271\n",
      "Epoch: 6 | Train loss: 0.2926 | Train acc: 0.9135 | Test loss: 0.2805 | Test acc: 0.9271\n",
      "Epoch: 7 | Train loss: 0.2901 | Train acc: 0.9062 | Test loss: 0.2741 | Test acc: 0.9062\n",
      "Epoch: 8 | Train loss: 0.2830 | Train acc: 0.9111 | Test loss: 0.3381 | Test acc: 0.8646\n",
      "Epoch: 9 | Train loss: 0.3052 | Train acc: 0.8966 | Test loss: 0.2702 | Test acc: 0.9271\n",
      "Epoch: 10 | Train loss: 0.2929 | Train acc: 0.8990 | Test loss: 0.2829 | Test acc: 0.8750\n",
      "Total training time: 0.586 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0.5, 0.5], covarient: 0.01\n",
      "Total generation time: 9.054 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3432 | Test acc: 0.9062\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3492 | Train acc: 0.8510 | Test loss: 0.3389 | Test acc: 0.8750\n",
      "Epoch: 2 | Train loss: 0.3348 | Train acc: 0.8389 | Test loss: 0.3529 | Test acc: 0.8542\n",
      "Epoch: 3 | Train loss: 0.3484 | Train acc: 0.8486 | Test loss: 0.3346 | Test acc: 0.8958\n",
      "Epoch: 4 | Train loss: 0.3361 | Train acc: 0.8582 | Test loss: 0.3339 | Test acc: 0.8958\n",
      "Epoch: 5 | Train loss: 0.3324 | Train acc: 0.8606 | Test loss: 0.3281 | Test acc: 0.8750\n",
      "Epoch: 6 | Train loss: 0.3408 | Train acc: 0.8678 | Test loss: 0.3293 | Test acc: 0.8958\n",
      "Epoch: 7 | Train loss: 0.3226 | Train acc: 0.8702 | Test loss: 0.3259 | Test acc: 0.8958\n",
      "Epoch: 8 | Train loss: 0.3163 | Train acc: 0.8630 | Test loss: 0.3212 | Test acc: 0.8958\n",
      "Epoch: 9 | Train loss: 0.3163 | Train acc: 0.8582 | Test loss: 0.3467 | Test acc: 0.8750\n",
      "Epoch: 10 | Train loss: 0.3376 | Train acc: 0.8389 | Test loss: 0.3150 | Test acc: 0.8958\n",
      "Total training time: 0.672 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0.5, 1], covarient: 0.01\n",
      "Total generation time: 8.576 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3831 | Test acc: 0.8125\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3276 | Train acc: 0.8654 | Test loss: 0.4104 | Test acc: 0.7917\n",
      "Epoch: 2 | Train loss: 0.3544 | Train acc: 0.8438 | Test loss: 0.3795 | Test acc: 0.8125\n",
      "Epoch: 3 | Train loss: 0.3277 | Train acc: 0.8630 | Test loss: 0.3792 | Test acc: 0.8125\n",
      "Epoch: 4 | Train loss: 0.3217 | Train acc: 0.8678 | Test loss: 0.3835 | Test acc: 0.8021\n",
      "Epoch: 5 | Train loss: 0.3035 | Train acc: 0.8918 | Test loss: 0.3636 | Test acc: 0.8021\n",
      "Epoch: 6 | Train loss: 0.3220 | Train acc: 0.8798 | Test loss: 0.3655 | Test acc: 0.8125\n",
      "Epoch: 7 | Train loss: 0.3180 | Train acc: 0.8630 | Test loss: 0.3608 | Test acc: 0.8125\n",
      "Epoch: 8 | Train loss: 0.3010 | Train acc: 0.8750 | Test loss: 0.3680 | Test acc: 0.7917\n",
      "Epoch: 9 | Train loss: 0.3018 | Train acc: 0.8942 | Test loss: 0.3513 | Test acc: 0.8125\n",
      "Epoch: 10 | Train loss: 0.2964 | Train acc: 0.8870 | Test loss: 0.3656 | Test acc: 0.7917\n",
      "Total training time: 0.585 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0.5, 1.5], covarient: 0.01\n",
      "Total generation time: 8.505 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.4405 | Test acc: 0.7708\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3685 | Train acc: 0.8221 | Test loss: 0.4374 | Test acc: 0.7812\n",
      "Epoch: 2 | Train loss: 0.3636 | Train acc: 0.8413 | Test loss: 0.4379 | Test acc: 0.7812\n",
      "Epoch: 3 | Train loss: 0.3681 | Train acc: 0.8365 | Test loss: 0.4297 | Test acc: 0.8021\n",
      "Epoch: 4 | Train loss: 0.3556 | Train acc: 0.8341 | Test loss: 0.4252 | Test acc: 0.8229\n",
      "Epoch: 5 | Train loss: 0.3548 | Train acc: 0.8413 | Test loss: 0.4221 | Test acc: 0.8021\n",
      "Epoch: 6 | Train loss: 0.3486 | Train acc: 0.8510 | Test loss: 0.4205 | Test acc: 0.8125\n",
      "Epoch: 7 | Train loss: 0.3481 | Train acc: 0.8438 | Test loss: 0.4282 | Test acc: 0.7917\n",
      "Epoch: 8 | Train loss: 0.3240 | Train acc: 0.8558 | Test loss: 0.4160 | Test acc: 0.8125\n",
      "Epoch: 9 | Train loss: 0.3379 | Train acc: 0.8726 | Test loss: 0.4153 | Test acc: 0.7917\n",
      "Epoch: 10 | Train loss: 0.3317 | Train acc: 0.8582 | Test loss: 0.4175 | Test acc: 0.8021\n",
      "Total training time: 0.632 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [0.5, 2], covarient: 0.01\n",
      "Total generation time: 8.837 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3273 | Test acc: 0.8229\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3258 | Train acc: 0.8702 | Test loss: 0.3277 | Test acc: 0.8333\n",
      "Epoch: 2 | Train loss: 0.3039 | Train acc: 0.8678 | Test loss: 0.3242 | Test acc: 0.8229\n",
      "Epoch: 3 | Train loss: 0.3122 | Train acc: 0.8654 | Test loss: 0.3270 | Test acc: 0.8542\n",
      "Epoch: 4 | Train loss: 0.3224 | Train acc: 0.8726 | Test loss: 0.3276 | Test acc: 0.8438\n",
      "Epoch: 5 | Train loss: 0.3008 | Train acc: 0.8846 | Test loss: 0.3184 | Test acc: 0.8333\n",
      "Epoch: 6 | Train loss: 0.2991 | Train acc: 0.8702 | Test loss: 0.3174 | Test acc: 0.8438\n",
      "Epoch: 7 | Train loss: 0.3152 | Train acc: 0.8750 | Test loss: 0.3177 | Test acc: 0.8542\n",
      "Epoch: 8 | Train loss: 0.3126 | Train acc: 0.8798 | Test loss: 0.3232 | Test acc: 0.8438\n",
      "Epoch: 9 | Train loss: 0.2978 | Train acc: 0.8750 | Test loss: 0.3125 | Test acc: 0.8333\n",
      "Epoch: 10 | Train loss: 0.2964 | Train acc: 0.8774 | Test loss: 0.3128 | Test acc: 0.8646\n",
      "Total training time: 0.512 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1, 0], covarient: 0.01\n",
      "Total generation time: 9.166 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2238 | Test acc: 0.9167\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2457 | Train acc: 0.9111 | Test loss: 0.2271 | Test acc: 0.9271\n",
      "Epoch: 2 | Train loss: 0.2476 | Train acc: 0.9111 | Test loss: 0.2228 | Test acc: 0.9167\n",
      "Epoch: 3 | Train loss: 0.2369 | Train acc: 0.9255 | Test loss: 0.2204 | Test acc: 0.9167\n",
      "Epoch: 4 | Train loss: 0.2494 | Train acc: 0.8966 | Test loss: 0.2222 | Test acc: 0.9271\n",
      "Epoch: 5 | Train loss: 0.2193 | Train acc: 0.9159 | Test loss: 0.2167 | Test acc: 0.9167\n",
      "Epoch: 6 | Train loss: 0.2250 | Train acc: 0.9111 | Test loss: 0.2172 | Test acc: 0.9167\n",
      "Epoch: 7 | Train loss: 0.2167 | Train acc: 0.9399 | Test loss: 0.2135 | Test acc: 0.9167\n",
      "Epoch: 8 | Train loss: 0.2211 | Train acc: 0.9111 | Test loss: 0.2129 | Test acc: 0.9167\n",
      "Epoch: 9 | Train loss: 0.2073 | Train acc: 0.9279 | Test loss: 0.2100 | Test acc: 0.9167\n",
      "Epoch: 10 | Train loss: 0.2076 | Train acc: 0.9327 | Test loss: 0.2130 | Test acc: 0.9271\n",
      "Total training time: 0.558 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1, 0.5], covarient: 0.01\n",
      "Total generation time: 8.696 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2308 | Test acc: 0.9271\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3129 | Train acc: 0.8750 | Test loss: 0.2248 | Test acc: 0.9375\n",
      "Epoch: 2 | Train loss: 0.2878 | Train acc: 0.8798 | Test loss: 0.2223 | Test acc: 0.9375\n",
      "Epoch: 3 | Train loss: 0.2986 | Train acc: 0.8966 | Test loss: 0.2216 | Test acc: 0.9375\n",
      "Epoch: 4 | Train loss: 0.2807 | Train acc: 0.9111 | Test loss: 0.2182 | Test acc: 0.9375\n",
      "Epoch: 5 | Train loss: 0.2947 | Train acc: 0.8870 | Test loss: 0.2202 | Test acc: 0.9271\n",
      "Epoch: 6 | Train loss: 0.2783 | Train acc: 0.9038 | Test loss: 0.2159 | Test acc: 0.9375\n",
      "Epoch: 7 | Train loss: 0.2871 | Train acc: 0.8798 | Test loss: 0.2145 | Test acc: 0.9583\n",
      "Epoch: 8 | Train loss: 0.2672 | Train acc: 0.9014 | Test loss: 0.2129 | Test acc: 0.9583\n",
      "Epoch: 9 | Train loss: 0.2601 | Train acc: 0.9014 | Test loss: 0.2131 | Test acc: 0.9375\n",
      "Epoch: 10 | Train loss: 0.2722 | Train acc: 0.8894 | Test loss: 0.2110 | Test acc: 0.9583\n",
      "Total training time: 0.643 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1, 1], covarient: 0.01\n",
      "Total generation time: 10.081 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2932 | Test acc: 0.8854\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3070 | Train acc: 0.8606 | Test loss: 0.3109 | Test acc: 0.8333\n",
      "Epoch: 2 | Train loss: 0.3281 | Train acc: 0.8413 | Test loss: 0.2919 | Test acc: 0.8854\n",
      "Epoch: 3 | Train loss: 0.2856 | Train acc: 0.8822 | Test loss: 0.2928 | Test acc: 0.8958\n",
      "Epoch: 4 | Train loss: 0.2943 | Train acc: 0.8654 | Test loss: 0.2946 | Test acc: 0.8958\n",
      "Epoch: 5 | Train loss: 0.2863 | Train acc: 0.8822 | Test loss: 0.2900 | Test acc: 0.8958\n",
      "Epoch: 6 | Train loss: 0.3027 | Train acc: 0.8438 | Test loss: 0.2901 | Test acc: 0.8958\n",
      "Epoch: 7 | Train loss: 0.2807 | Train acc: 0.8750 | Test loss: 0.2863 | Test acc: 0.8958\n",
      "Epoch: 8 | Train loss: 0.2808 | Train acc: 0.9014 | Test loss: 0.2849 | Test acc: 0.8958\n",
      "Epoch: 9 | Train loss: 0.2762 | Train acc: 0.8846 | Test loss: 0.2871 | Test acc: 0.8958\n",
      "Epoch: 10 | Train loss: 0.2862 | Train acc: 0.8678 | Test loss: 0.2838 | Test acc: 0.8958\n",
      "Total training time: 0.767 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1, 1.5], covarient: 0.01\n",
      "Total generation time: 11.127 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2493 | Test acc: 0.9271\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3168 | Train acc: 0.8654 | Test loss: 0.2455 | Test acc: 0.9167\n",
      "Epoch: 2 | Train loss: 0.3152 | Train acc: 0.8750 | Test loss: 0.2443 | Test acc: 0.9271\n",
      "Epoch: 3 | Train loss: 0.3199 | Train acc: 0.8678 | Test loss: 0.2418 | Test acc: 0.9479\n",
      "Epoch: 4 | Train loss: 0.2873 | Train acc: 0.8774 | Test loss: 0.2455 | Test acc: 0.9167\n",
      "Epoch: 5 | Train loss: 0.3077 | Train acc: 0.8750 | Test loss: 0.2393 | Test acc: 0.9167\n",
      "Epoch: 6 | Train loss: 0.3085 | Train acc: 0.8846 | Test loss: 0.2419 | Test acc: 0.9167\n",
      "Epoch: 7 | Train loss: 0.2748 | Train acc: 0.9159 | Test loss: 0.2375 | Test acc: 0.9375\n",
      "Epoch: 8 | Train loss: 0.2989 | Train acc: 0.8750 | Test loss: 0.2371 | Test acc: 0.9479\n",
      "Epoch: 9 | Train loss: 0.3101 | Train acc: 0.8630 | Test loss: 0.2356 | Test acc: 0.9167\n",
      "Epoch: 10 | Train loss: 0.2717 | Train acc: 0.8942 | Test loss: 0.2357 | Test acc: 0.9375\n",
      "Total training time: 0.631 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1, 2], covarient: 0.01\n",
      "Total generation time: 8.802 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3149 | Test acc: 0.8438\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2821 | Train acc: 0.8894 | Test loss: 0.3192 | Test acc: 0.8854\n",
      "Epoch: 2 | Train loss: 0.2937 | Train acc: 0.8870 | Test loss: 0.3145 | Test acc: 0.8438\n",
      "Epoch: 3 | Train loss: 0.2864 | Train acc: 0.8798 | Test loss: 0.3144 | Test acc: 0.8438\n",
      "Epoch: 4 | Train loss: 0.2679 | Train acc: 0.8774 | Test loss: 0.3163 | Test acc: 0.8750\n",
      "Epoch: 5 | Train loss: 0.2846 | Train acc: 0.8894 | Test loss: 0.3154 | Test acc: 0.8646\n",
      "Epoch: 6 | Train loss: 0.2510 | Train acc: 0.9159 | Test loss: 0.3143 | Test acc: 0.8646\n",
      "Epoch: 7 | Train loss: 0.2662 | Train acc: 0.8966 | Test loss: 0.3138 | Test acc: 0.8646\n",
      "Epoch: 8 | Train loss: 0.2712 | Train acc: 0.8966 | Test loss: 0.3137 | Test acc: 0.8646\n",
      "Epoch: 9 | Train loss: 0.2766 | Train acc: 0.8894 | Test loss: 0.3139 | Test acc: 0.8646\n",
      "Epoch: 10 | Train loss: 0.2527 | Train acc: 0.9111 | Test loss: 0.3149 | Test acc: 0.8646\n",
      "Total training time: 0.569 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1.5, 0], covarient: 0.01\n",
      "Total generation time: 8.758 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.1780 | Test acc: 0.9688\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2168 | Train acc: 0.9159 | Test loss: 0.1772 | Test acc: 0.9583\n",
      "Epoch: 2 | Train loss: 0.2350 | Train acc: 0.8990 | Test loss: 0.1653 | Test acc: 0.9688\n",
      "Epoch: 3 | Train loss: 0.2442 | Train acc: 0.9087 | Test loss: 0.1745 | Test acc: 0.9583\n",
      "Epoch: 4 | Train loss: 0.2323 | Train acc: 0.9159 | Test loss: 0.1623 | Test acc: 0.9792\n",
      "Epoch: 5 | Train loss: 0.2257 | Train acc: 0.9062 | Test loss: 0.1618 | Test acc: 0.9688\n",
      "Epoch: 6 | Train loss: 0.2097 | Train acc: 0.9399 | Test loss: 0.1599 | Test acc: 0.9688\n",
      "Epoch: 7 | Train loss: 0.2103 | Train acc: 0.9375 | Test loss: 0.1579 | Test acc: 0.9688\n",
      "Epoch: 8 | Train loss: 0.2231 | Train acc: 0.8990 | Test loss: 0.1576 | Test acc: 0.9688\n",
      "Epoch: 9 | Train loss: 0.2182 | Train acc: 0.9087 | Test loss: 0.1556 | Test acc: 0.9688\n",
      "Epoch: 10 | Train loss: 0.2165 | Train acc: 0.9087 | Test loss: 0.1536 | Test acc: 0.9896\n",
      "Total training time: 0.554 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1.5, 0.5], covarient: 0.01\n",
      "Total generation time: 8.798 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3686 | Test acc: 0.8438\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2805 | Train acc: 0.8774 | Test loss: 0.3803 | Test acc: 0.7917\n",
      "Epoch: 2 | Train loss: 0.2786 | Train acc: 0.8678 | Test loss: 0.3697 | Test acc: 0.8125\n",
      "Epoch: 3 | Train loss: 0.2715 | Train acc: 0.8966 | Test loss: 0.3674 | Test acc: 0.8125\n",
      "Epoch: 4 | Train loss: 0.2926 | Train acc: 0.8510 | Test loss: 0.4009 | Test acc: 0.7917\n",
      "Epoch: 5 | Train loss: 0.2897 | Train acc: 0.8798 | Test loss: 0.3699 | Test acc: 0.8542\n",
      "Epoch: 6 | Train loss: 0.2713 | Train acc: 0.8870 | Test loss: 0.3961 | Test acc: 0.7917\n",
      "Epoch: 7 | Train loss: 0.2720 | Train acc: 0.8822 | Test loss: 0.3697 | Test acc: 0.8125\n",
      "Epoch: 8 | Train loss: 0.2617 | Train acc: 0.8990 | Test loss: 0.3676 | Test acc: 0.8333\n",
      "Epoch: 9 | Train loss: 0.2800 | Train acc: 0.8894 | Test loss: 0.3809 | Test acc: 0.8125\n",
      "Epoch: 10 | Train loss: 0.2623 | Train acc: 0.8870 | Test loss: 0.3701 | Test acc: 0.8021\n",
      "Total training time: 0.609 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1.5, 1], covarient: 0.01\n",
      "Total generation time: 8.872 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2773 | Test acc: 0.8854\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2834 | Train acc: 0.8870 | Test loss: 0.2802 | Test acc: 0.8646\n",
      "Epoch: 2 | Train loss: 0.2798 | Train acc: 0.8846 | Test loss: 0.2765 | Test acc: 0.8750\n",
      "Epoch: 3 | Train loss: 0.2661 | Train acc: 0.8798 | Test loss: 0.2746 | Test acc: 0.8854\n",
      "Epoch: 4 | Train loss: 0.2520 | Train acc: 0.9038 | Test loss: 0.2734 | Test acc: 0.8750\n",
      "Epoch: 5 | Train loss: 0.2888 | Train acc: 0.8774 | Test loss: 0.2730 | Test acc: 0.8750\n",
      "Epoch: 6 | Train loss: 0.2837 | Train acc: 0.8846 | Test loss: 0.2706 | Test acc: 0.8854\n",
      "Epoch: 7 | Train loss: 0.2733 | Train acc: 0.8846 | Test loss: 0.2686 | Test acc: 0.8750\n",
      "Epoch: 8 | Train loss: 0.2544 | Train acc: 0.8894 | Test loss: 0.2678 | Test acc: 0.8750\n",
      "Epoch: 9 | Train loss: 0.2702 | Train acc: 0.8942 | Test loss: 0.2671 | Test acc: 0.8750\n",
      "Epoch: 10 | Train loss: 0.2561 | Train acc: 0.8942 | Test loss: 0.2675 | Test acc: 0.8750\n",
      "Total training time: 0.649 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1.5, 1.5], covarient: 0.01\n",
      "Total generation time: 8.639 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2526 | Test acc: 0.8854\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3220 | Train acc: 0.8462 | Test loss: 0.2472 | Test acc: 0.9062\n",
      "Epoch: 2 | Train loss: 0.3140 | Train acc: 0.8654 | Test loss: 0.2501 | Test acc: 0.8854\n",
      "Epoch: 3 | Train loss: 0.3182 | Train acc: 0.8534 | Test loss: 0.2498 | Test acc: 0.8854\n",
      "Epoch: 4 | Train loss: 0.3017 | Train acc: 0.8678 | Test loss: 0.2451 | Test acc: 0.8958\n",
      "Epoch: 5 | Train loss: 0.2850 | Train acc: 0.8678 | Test loss: 0.2445 | Test acc: 0.8958\n",
      "Epoch: 6 | Train loss: 0.3079 | Train acc: 0.8654 | Test loss: 0.2474 | Test acc: 0.8958\n",
      "Epoch: 7 | Train loss: 0.3144 | Train acc: 0.8702 | Test loss: 0.2425 | Test acc: 0.9062\n",
      "Epoch: 8 | Train loss: 0.2870 | Train acc: 0.8846 | Test loss: 0.2433 | Test acc: 0.8958\n",
      "Epoch: 9 | Train loss: 0.2783 | Train acc: 0.8846 | Test loss: 0.2413 | Test acc: 0.9062\n",
      "Epoch: 10 | Train loss: 0.2707 | Train acc: 0.9014 | Test loss: 0.2411 | Test acc: 0.9062\n",
      "Total training time: 0.705 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [1.5, 2], covarient: 0.01\n",
      "Total generation time: 8.895 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3383 | Test acc: 0.8333\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3063 | Train acc: 0.8750 | Test loss: 0.3261 | Test acc: 0.8542\n",
      "Epoch: 2 | Train loss: 0.3181 | Train acc: 0.8630 | Test loss: 0.3243 | Test acc: 0.8542\n",
      "Epoch: 3 | Train loss: 0.2981 | Train acc: 0.8798 | Test loss: 0.3231 | Test acc: 0.8542\n",
      "Epoch: 4 | Train loss: 0.3152 | Train acc: 0.8582 | Test loss: 0.3309 | Test acc: 0.8333\n",
      "Epoch: 5 | Train loss: 0.3086 | Train acc: 0.8534 | Test loss: 0.3232 | Test acc: 0.8542\n",
      "Epoch: 6 | Train loss: 0.3005 | Train acc: 0.8630 | Test loss: 0.3273 | Test acc: 0.8438\n",
      "Epoch: 7 | Train loss: 0.2949 | Train acc: 0.8750 | Test loss: 0.3253 | Test acc: 0.8333\n",
      "Epoch: 8 | Train loss: 0.2944 | Train acc: 0.8630 | Test loss: 0.3230 | Test acc: 0.8333\n",
      "Epoch: 9 | Train loss: 0.2936 | Train acc: 0.8750 | Test loss: 0.3232 | Test acc: 0.8333\n",
      "Epoch: 10 | Train loss: 0.2833 | Train acc: 0.8846 | Test loss: 0.3234 | Test acc: 0.8333\n",
      "Total training time: 0.509 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [2, 0], covarient: 0.01\n",
      "Total generation time: 8.678 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.1136 | Test acc: 0.9896\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2021 | Train acc: 0.9255 | Test loss: 0.1216 | Test acc: 0.9688\n",
      "Epoch: 2 | Train loss: 0.2096 | Train acc: 0.9255 | Test loss: 0.1138 | Test acc: 0.9688\n",
      "Epoch: 3 | Train loss: 0.1981 | Train acc: 0.9279 | Test loss: 0.1122 | Test acc: 0.9688\n",
      "Epoch: 4 | Train loss: 0.2053 | Train acc: 0.9255 | Test loss: 0.1110 | Test acc: 0.9688\n",
      "Epoch: 5 | Train loss: 0.1893 | Train acc: 0.9471 | Test loss: 0.1122 | Test acc: 0.9688\n",
      "Epoch: 6 | Train loss: 0.1939 | Train acc: 0.9279 | Test loss: 0.1067 | Test acc: 0.9688\n",
      "Epoch: 7 | Train loss: 0.1769 | Train acc: 0.9423 | Test loss: 0.1078 | Test acc: 0.9688\n",
      "Epoch: 8 | Train loss: 0.1741 | Train acc: 0.9375 | Test loss: 0.1092 | Test acc: 0.9688\n",
      "Epoch: 9 | Train loss: 0.1834 | Train acc: 0.9303 | Test loss: 0.1078 | Test acc: 0.9688\n",
      "Epoch: 10 | Train loss: 0.1802 | Train acc: 0.9279 | Test loss: 0.1032 | Test acc: 0.9688\n",
      "Total training time: 0.655 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [2, 0.5], covarient: 0.01\n",
      "Total generation time: 8.522 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2635 | Test acc: 0.8750\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2704 | Train acc: 0.9038 | Test loss: 0.2593 | Test acc: 0.8854\n",
      "Epoch: 2 | Train loss: 0.2871 | Train acc: 0.8774 | Test loss: 0.2585 | Test acc: 0.8958\n",
      "Epoch: 3 | Train loss: 0.2643 | Train acc: 0.8894 | Test loss: 0.2595 | Test acc: 0.8958\n",
      "Epoch: 4 | Train loss: 0.2517 | Train acc: 0.8990 | Test loss: 0.2610 | Test acc: 0.8646\n",
      "Epoch: 5 | Train loss: 0.2717 | Train acc: 0.8846 | Test loss: 0.2628 | Test acc: 0.8854\n",
      "Epoch: 6 | Train loss: 0.2455 | Train acc: 0.8774 | Test loss: 0.2610 | Test acc: 0.8958\n",
      "Epoch: 7 | Train loss: 0.2420 | Train acc: 0.9159 | Test loss: 0.2607 | Test acc: 0.8854\n",
      "Epoch: 8 | Train loss: 0.2312 | Train acc: 0.8966 | Test loss: 0.2657 | Test acc: 0.8750\n",
      "Epoch: 9 | Train loss: 0.2578 | Train acc: 0.8942 | Test loss: 0.2618 | Test acc: 0.8854\n",
      "Epoch: 10 | Train loss: 0.2403 | Train acc: 0.9183 | Test loss: 0.2631 | Test acc: 0.8958\n",
      "Total training time: 0.703 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [2, 1], covarient: 0.01\n",
      "Total generation time: 9.118 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2240 | Test acc: 0.9167\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2994 | Train acc: 0.8750 | Test loss: 0.2397 | Test acc: 0.9167\n",
      "Epoch: 2 | Train loss: 0.2812 | Train acc: 0.8846 | Test loss: 0.2253 | Test acc: 0.9167\n",
      "Epoch: 3 | Train loss: 0.2836 | Train acc: 0.8966 | Test loss: 0.2380 | Test acc: 0.9167\n",
      "Epoch: 4 | Train loss: 0.2892 | Train acc: 0.8774 | Test loss: 0.2303 | Test acc: 0.9167\n",
      "Epoch: 5 | Train loss: 0.2718 | Train acc: 0.8870 | Test loss: 0.2318 | Test acc: 0.9062\n",
      "Epoch: 6 | Train loss: 0.2706 | Train acc: 0.8798 | Test loss: 0.2244 | Test acc: 0.9167\n",
      "Epoch: 7 | Train loss: 0.2839 | Train acc: 0.8846 | Test loss: 0.2310 | Test acc: 0.9271\n",
      "Epoch: 8 | Train loss: 0.2488 | Train acc: 0.9062 | Test loss: 0.2243 | Test acc: 0.9167\n",
      "Epoch: 9 | Train loss: 0.2259 | Train acc: 0.9279 | Test loss: 0.2284 | Test acc: 0.9062\n",
      "Epoch: 10 | Train loss: 0.2739 | Train acc: 0.8750 | Test loss: 0.2241 | Test acc: 0.9167\n",
      "Total training time: 0.620 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [2, 1.5], covarient: 0.01\n",
      "Total generation time: 8.832 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.2802 | Test acc: 0.8854\n",
      "\n",
      "Epoch: 1 | Train loss: 0.2760 | Train acc: 0.8774 | Test loss: 0.2768 | Test acc: 0.8750\n",
      "Epoch: 2 | Train loss: 0.2627 | Train acc: 0.8894 | Test loss: 0.2865 | Test acc: 0.8854\n",
      "Epoch: 3 | Train loss: 0.2877 | Train acc: 0.8822 | Test loss: 0.2715 | Test acc: 0.8750\n",
      "Epoch: 4 | Train loss: 0.2564 | Train acc: 0.8966 | Test loss: 0.2769 | Test acc: 0.8854\n",
      "Epoch: 5 | Train loss: 0.2661 | Train acc: 0.8798 | Test loss: 0.2791 | Test acc: 0.8854\n",
      "Epoch: 6 | Train loss: 0.2414 | Train acc: 0.8822 | Test loss: 0.2695 | Test acc: 0.8750\n",
      "Epoch: 7 | Train loss: 0.2445 | Train acc: 0.8918 | Test loss: 0.2777 | Test acc: 0.8854\n",
      "Epoch: 8 | Train loss: 0.2438 | Train acc: 0.8894 | Test loss: 0.2711 | Test acc: 0.8750\n",
      "Epoch: 9 | Train loss: 0.2330 | Train acc: 0.9087 | Test loss: 0.2762 | Test acc: 0.8854\n",
      "Epoch: 10 | Train loss: 0.2373 | Train acc: 0.8990 | Test loss: 0.2802 | Test acc: 0.9062\n",
      "Total training time: 0.563 seconds\n",
      "Total parameters: 32930\n",
      "\n",
      "\n",
      "variance a:0.3, variance b:0.3, mean: [2, 2], covarient: 0.01\n",
      "Total generation time: 9.094 seconds\n",
      "Training data shape: (400, 64, 64)\n",
      "Validation data shape: (80, 64, 64)\n",
      "\n",
      " Model performance before training | Test loss: 0.3279 | Test acc: 0.8125\n",
      "\n",
      "Epoch: 1 | Train loss: 0.3408 | Train acc: 0.8678 | Test loss: 0.3279 | Test acc: 0.8229\n",
      "Epoch: 2 | Train loss: 0.3531 | Train acc: 0.8654 | Test loss: 0.3134 | Test acc: 0.8229\n",
      "Epoch: 3 | Train loss: 0.3326 | Train acc: 0.8678 | Test loss: 0.3278 | Test acc: 0.8125\n",
      "Epoch: 4 | Train loss: 0.3272 | Train acc: 0.8678 | Test loss: 0.3131 | Test acc: 0.8229\n",
      "Epoch: 5 | Train loss: 0.3352 | Train acc: 0.8510 | Test loss: 0.3092 | Test acc: 0.8333\n",
      "Epoch: 6 | Train loss: 0.3137 | Train acc: 0.8702 | Test loss: 0.3099 | Test acc: 0.8438\n",
      "Epoch: 7 | Train loss: 0.3046 | Train acc: 0.8750 | Test loss: 0.3069 | Test acc: 0.8438\n",
      "Epoch: 8 | Train loss: 0.3329 | Train acc: 0.8582 | Test loss: 0.3141 | Test acc: 0.8333\n",
      "Epoch: 9 | Train loss: 0.3093 | Train acc: 0.8702 | Test loss: 0.3035 | Test acc: 0.8333\n",
      "Epoch: 10 | Train loss: 0.2958 | Train acc: 0.8870 | Test loss: 0.3126 | Test acc: 0.8438\n",
      "Total training time: 0.655 seconds\n",
      "Total parameters: 32930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Change Varience, same mean at 1,1, not log normal\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from typing import Dict, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import platform\n",
    "from scipy.stats import multivariate_normal, lognorm, expon, powerlaw\n",
    "from sklearn.utils import shuffle\n",
    "import os \n",
    "\n",
    "# Import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check version\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}\\n')\n",
    "\n",
    "# setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    gpu_memory_info = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_memory_info.total_memory / (1024 ** 3)  # Convert to GB\n",
    "    free_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n",
    "    used_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"\\nTotal GPU Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Free GPU Memory: {free_memory:.2f} GB\")\n",
    "    print(f\"Used GPU Memory: {used_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\t\n",
    "def generate_ratios(mean, cov, distribution='gaussian'):\n",
    "    if distribution == 'lognormal':\n",
    "        # Use the mean and standard deviation to create log-normal distribution\n",
    "        mean_a_c, mean_b_c = mean\n",
    "        var_a_c, var_b_c = np.diag(cov)\n",
    "        s_a_c = np.sqrt(var_a_c)\n",
    "        s_b_c = np.sqrt(var_b_c)\n",
    "        \n",
    "        # Generate a/c and b/c from log-normal distributions\n",
    "        a_c = lognorm.rvs(s=s_a_c, scale=np.exp(mean_a_c))\n",
    "        b_c = lognorm.rvs(s=s_b_c, scale=np.exp(mean_b_c))\n",
    "    \n",
    "    elif distribution == 'exponential':\n",
    "        # Use the mean as the scale parameter for exponential distribution\n",
    "        a_c = expon.rvs(scale=mean[0])\n",
    "        b_c = expon.rvs(scale=mean[1])\n",
    "    \n",
    "    elif distribution == 'powerlaw':\n",
    "        # Generate a/c and b/c from a power law distribution\n",
    "        # The parameter `a` controls the shape of the distribution\n",
    "        a_c = powerlaw.rvs(a=mean[0], scale=1)\n",
    "        b_c = powerlaw.rvs(a=mean[1], scale=1)\n",
    "    \n",
    "    else:  # Gaussian by default\n",
    "        # Generate ratios from a 2D Gaussian distribution\n",
    "        a_c, b_c = multivariate_normal.rvs(mean, cov)\n",
    "    \n",
    "    return a_c, b_c\n",
    "\n",
    "\"\"\" Functions that are used to generate filled sphere and\n",
    "elipsoid\"\"\"\n",
    "\n",
    "# Generate points on surface of sphere\n",
    "def sphere(radius, num_points, plot=False):\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points) * radius)\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    x = r * np.sin(phi) * np.cos(theta)\n",
    "    y = r * np.sin(phi) * np.sin(theta)\n",
    "    z = r * np.cos(phi)\n",
    "\n",
    "    if plot:\n",
    "        # Plot Structure\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        plt.gca().set_aspect('auto', adjustable='box')\n",
    "        ax.scatter(x,y,z, marker='.')\n",
    "        ax.set_aspect('equal', 'box') #auto adjust limits\n",
    "        #ax.axis('equal')\n",
    "        ax.set_title('Structure of Circle', fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def ellipsoid(radius, num_points, mean, cov, distribution='gaussian', plot=False):\n",
    "    # Generate random angles and radius for spherical coordinates\n",
    "    u = np.random.rand(num_points) \n",
    "    v = np.random.rand(num_points) \n",
    "    r = np.cbrt(np.random.rand(num_points))\n",
    "     \n",
    "    theta = u * 2 * np.pi\n",
    "    phi = v * np.pi\n",
    "\n",
    "    # Generate a/c and b/c ratios\n",
    "    a_c, b_c = generate_ratios(mean, cov, distribution)\n",
    "\n",
    "    # Generate ellipsoid coordinates with the scaling factors\n",
    "    x = a_c * radius * r * np.sin(phi) * np.cos(theta)\n",
    "    y = b_c * radius * r * np.sin(phi) * np.sin(theta)\n",
    "    z = radius * r * np.cos(phi)  # Here c is set to radius\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x, y, z, marker='.')\n",
    "        ax.set_title(f'Ellipsoid with a/c={a_c:.2f}, b/c={b_c:.2f}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Return points as list of tuples\n",
    "    points = np.column_stack((x.flatten(), y.flatten(), z.flatten()))\n",
    "\n",
    "    return points\n",
    "\n",
    "def points_projection(structure_coords, num_points):\n",
    "    \"\"\" \n",
    "    Functions for projection\n",
    "    \"\"\"\n",
    "    # Assign structure coords into z\n",
    "    z = structure_coords\n",
    "    \n",
    "    # normal vectors generation\n",
    "    normal = sphere(1, num_points)\n",
    "\n",
    "    all_projected_points = []\n",
    "    for n in normal:\n",
    "        #Find two orthogonal vectors u and v (both orthogonal to n)\n",
    "        #Calc value for t (random vector), ensuring not a scaled version of n\n",
    "        if n[0] != 0:\n",
    "            t = np.array([-(n[1]+n[2]) / n[0], 1, 1])\n",
    "        elif n[1] != 0:\n",
    "            t = np.array([-(n[0]+n[2]) / n[1], 1, 1])\n",
    "        else:\n",
    "            t = np.array([-(n[0]+n[1]) / n[2], 1, 1])\n",
    "        \n",
    "        u = np.cross(t,n)\n",
    "        v = np.cross(n,u)\n",
    "        \n",
    "        # Normalize u and v (vector length become 1 unit long)\n",
    "        u = u / np.linalg.norm(u)\n",
    "        v = v / np.linalg.norm(v)\n",
    "        \n",
    "        vec_mat = np.array([u,v])\n",
    "        \n",
    "        #Project structure points onto plane\n",
    "        #Individual component of normal\n",
    "        a = n[0]\n",
    "        b = n[1]\n",
    "        c = n[2]\n",
    "        #d = 0 #component of equation of planes\n",
    "\n",
    "        projected_points = []\n",
    "        for point in z:\n",
    "            z1, z2, z3 = point\n",
    "            \n",
    "            k = (0 - a*z1 - b*z2 - c*z3) / (a**2 + b**2 + c**2) \n",
    "            \n",
    "            p1 = z1 + k*a\n",
    "            p2 = z2 + k*b\n",
    "            p3 = z3 + k*c\n",
    "            \n",
    "            p = np.array([p1,p2,p3])\n",
    "\n",
    "            #Convert 3D points to 2D\n",
    "            p_trans = p.transpose()\n",
    "            proj_2d = np.dot(vec_mat,p_trans)\n",
    "            projected_points.append(proj_2d)\n",
    "            \n",
    "        all_projected_points.append(projected_points)\n",
    "\n",
    "    return np.array(all_projected_points)\n",
    "\n",
    "\n",
    "def cluster_per_cell(projected_points, image_size, grid_size):\n",
    "    '''\n",
    "    Functiom that transforms projections into grid and no of points\n",
    "    '''\n",
    "    all_projections = np.array(projected_points)\n",
    "    image_size = image_size\n",
    "    grid_x = grid_size[0]\n",
    "    grid_y = grid_size[1]\n",
    "    \n",
    "    #Calc size of grid cell\n",
    "    cell_x = image_size[0] / grid_x\n",
    "    cell_y = image_size[1] / grid_y\n",
    "\n",
    "    all_grid = []\n",
    "    for projection in all_projections:\n",
    "        grid = np.zeros((grid_x,grid_y), dtype=int)\n",
    "        \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projection, axis=0)\n",
    "        max_val = np.max(projection, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projection - min_val) / (max_val - min_val) \n",
    "        \n",
    "        scaled_points = (points_norm * (np.array(image_size) - 1)).astype(int)\n",
    "        \n",
    "        for points in scaled_points:\n",
    "            x,y = points\n",
    "            gridx_index = int(x // cell_x) #floor division followed by conversion to integer\n",
    "            gridy_index = int(y // cell_y)\n",
    "            grid[gridy_index, gridx_index] += 1\n",
    "            \n",
    "        all_grid.append(grid)\n",
    "        \n",
    "    # transform into bw image \n",
    "    all_images = []\n",
    "    for grid_img in all_grid:\n",
    "        min = np.min(grid_img)\n",
    "        max = np.max(grid_img)\n",
    "        points_norm = (grid_img - min) / (max - min) \n",
    "        all_images.append(points_norm)\n",
    "\n",
    "    return  all_images\n",
    "\n",
    "\n",
    "def image_projection(coords, size):\n",
    "    '''\n",
    "    # Transform projected points into image with 1s and 0s\n",
    "    '''\n",
    "    all_projects = np.array(coords)\n",
    "    image_size = size\n",
    "\n",
    "    all_images = []\n",
    "    for projects in all_projects:\n",
    "    \n",
    "        #Normalise 2D coords for better scalling (between 0-1)\n",
    "        min_val = np.min(projects, axis=0)\n",
    "        max_val = np.max(projects, axis=0)\n",
    "        \n",
    "        #Feature scaling \n",
    "        points_norm = (projects - min_val) / (max_val - min_val) \n",
    "        \n",
    "        # Scale points to image size\n",
    "        points_scaled = (points_norm * (np.array(image_size) -1 )).astype(int)\n",
    "        \n",
    "        # Create an empty image\n",
    "        image = np.zeros(image_size)\n",
    "        \n",
    "        # Populate the image with points\n",
    "        for point in points_scaled:\n",
    "            x, y = point\n",
    "            image[y,x] = 1  # Note: (y, x) because image coordinates are row-major\n",
    "        \n",
    "        all_images.append(image)\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "\n",
    "#Function that makes labels\n",
    "def label_making(label_num, lst):\n",
    "    label = [label_num] * len(lst)\n",
    "    return label\n",
    "\n",
    "def rotation(structure):\n",
    "    point_cloud = structure\n",
    "    \n",
    "    # Convert to homogeneous coordinates\n",
    "    point_cloud_homogeneous = []\n",
    "    for point in structure:\n",
    "        point_homogeneous = np.append(point,1)\n",
    "        point_cloud_homogeneous.append(point_homogeneous)\n",
    "    \n",
    "    x, y, z= np.random.uniform(low = 0, high = 2 * np.pi, size=3)\n",
    "    \n",
    "    cx, sx = np.cos(x), np.sin(x)\n",
    "    cy, sy = np.cos(y), np.sin(y)\n",
    "    cz, sz = np.cos(z), np.sin(z)\n",
    "    \n",
    "    rotate_x = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, cx, -sx, 0],\n",
    "        [0, sx, cx, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_y = np.array([\n",
    "        [cy, 0, sy, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-sy, 0, cy, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    rotate_z = np.array([\n",
    "        [cy, -sy, 0, 0],\n",
    "        [-sy, cy, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        ])\n",
    "\n",
    "    # Rotate in a 3 axis\n",
    "    rotated_points = np.matmul(\n",
    "        point_cloud_homogeneous,\n",
    "        rotate_x)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_y)\n",
    "    \n",
    "    rotated_points = np.matmul(\n",
    "        rotated_points,\n",
    "        rotate_z)\n",
    "    \n",
    "    # Convert to cartesian coordinates\n",
    "    rotated_points_xyz = []\n",
    "    for point in rotated_points:\n",
    "        point = np.array(point[:-1])\n",
    "        rotated_points_xyz.append(point)\n",
    "\n",
    "    return np.array(rotated_points_xyz)\n",
    "\n",
    "    \n",
    "def plot_images(image1, image2, image3, image4, filename):\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    images = [image1, image2, image3, image4]\n",
    "    titles = ['Sphere System 1', 'Sphere System 2', 'Ellipsoid System 1', 'Ellipsoid System 2']\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        # Determine the position in the 2x2 grid\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Set the predefined title for each subplot\n",
    "        ax.set_title(titles[idx], fontsize=10)\n",
    "        \n",
    "        ax.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def plot_loss_curves(results: Dict[str, List[float]],filename):\n",
    "    \"\"\" Plots training curves of a result dictionary \"\"\"\n",
    "    # Get loss value of result dictionary(training and testing)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    # Get accuracy values of the result dictionary (training and testing)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    #Figure out no of epochs\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    \n",
    "    #Setup plot\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the loss\n",
    "    ax[0].plot(epochs, loss, label=\"Train_loss\")\n",
    "    ax[0].plot(epochs, test_loss, label=\"Test_loss\")\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the accuracy\n",
    "    ax[1].plot(epochs, accuracy, label=\"Train_accuracy\")\n",
    "    ax[1].plot(epochs, test_accuracy, label=\"Test_accuracy\")\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# takes in data and labels to transform into dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label\n",
    "        \n",
    "# CNN model    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, padding_mode='circular')\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "print('Model being used is SimpleCNN')\n",
    "\n",
    "# create train_step()\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    \n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader, data batch\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to the target device\n",
    "        X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "        #1. Forward pass\n",
    "        y_pred = model(X) #output model logits\n",
    "        \n",
    "        #2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #6. Calculate accuracy metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred) # total no correct divided by len of sample\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# create test_step\n",
    "def test_step(model:  torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    #Setup test loss and test accuract values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inderence mode\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device).unsqueeze(1), y.to(device)\n",
    "\n",
    "            #1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            #2. Calculate the loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            #3. Calculate the accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    #Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss /  len(dataloader)\n",
    "    test_acc = test_acc /  len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "# Create train function\n",
    "#1. Create a train function that takes in varius model parameters + optimizer + dataloaders\n",
    "def train(model:torch.nn.Module,\n",
    "          train_data: torch.utils.data.DataLoader,\n",
    "          test_data: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 10,\n",
    "          device=device):\n",
    "\n",
    "    #Create result dictionary\n",
    "    results = {'train_loss': [],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "    # Loop through training and testing steps for x number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_data,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_data,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        #Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "\n",
    "        #Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res, distance):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_spheres = np.random.randint(5,max_spheres) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_spheres):\n",
    "            a = sphere(max_sphere_size, no_of_points) # create sphere\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1,distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\n",
    "\n",
    "def system_maker2(no_of_systems ,max_ellipsoids, max_ellipsoid_size, no_of_points, no_of_projections, image_res, distance, distribution_type='gaussian'):\n",
    "    bw_img_all = []\n",
    "    grid_img_all = []\n",
    "    for _ in range(no_of_systems):\n",
    "        systems = []\n",
    "        no_of_ellipsoids = np.random.randint(5,max_ellipsoids) # min 5 sphere per system\n",
    "        #no_of_spheres = max_spheres\n",
    "        for _ in range(no_of_ellipsoids):\n",
    "            a = ellipsoid(radius=max_ellipsoid_size, num_points=no_of_points, mean=mean, cov=cov, distribution=distribution_type)\n",
    "            a = rotation(a)\n",
    "            a = a + (np.random.rand(1,3) * np.random.randint(1, distance)) # translate sphere around\n",
    "            systems.append(a) # add spheres\n",
    "        \n",
    "        systems = np.array(systems) #transform into numpy array\n",
    "        systems = systems.reshape(-1 , systems.shape[-1]) # reshape so that spheres coordinates in each systems combines\n",
    "\n",
    "        # Project 3D system ontto 2D plane\n",
    "        proj_2D = points_projection(systems,no_of_projections)\n",
    "        \n",
    "        # Transform 2D points into 1s ad 0s image & 0-1 range image\n",
    "        #image_bw = image_projection(proj_2D, image_res)\n",
    "        image_contrast = cluster_per_cell(proj_2D, (720, 720), image_res)\n",
    "\n",
    "        #bw_img_all.append(image_bw)\n",
    "        grid_img_all.append(image_contrast)\n",
    "\n",
    "    #bw_img_all = np.array(bw_img_all)\n",
    "    grid_img_all = np.array(grid_img_all)\n",
    "    #bw_img_all = bw_img_all.reshape(-1,bw_img_all.shape[-2],bw_img_all.shape[-1] )\n",
    "    grid_img_all = grid_img_all.reshape(-1,grid_img_all.shape[-2],grid_img_all.shape[-1] )\n",
    "\n",
    "    return grid_img_all #bw_img_all\t\n",
    "\n",
    "## Test script\n",
    "cov_ab = 0.01  # Covariance between a/c and b/c\n",
    "variance_a = 0.3  # Variance for a/c\n",
    "variance_b = 0.3  # Variance for b/c\n",
    "\n",
    "mean_a = mean_b = [0, 0.5, 1, 1.5, 2]\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "\n",
    "d = 40\n",
    "image_name = 'meanL_img'\n",
    "loss_name = 'meanL_loss'\n",
    "for i in range(len(mean_a)):\n",
    "    for j in range(len(mean_b)):\n",
    "        mean = [mean_a[i], mean_b[j]] # a/c, b/c\n",
    "        start = timer()\n",
    "        '''system_maker(no_of_systems ,max_spheres, max_sphere_size, no_of_points, no_of_projections, image_res)'''\n",
    "        \n",
    "            # Generate training images\n",
    "        sphere_img_train = system_maker(100, 30, 1, 100, 2, (64, 64),d)\n",
    "        ellips_img_train = system_maker2(100, 30, 1, 100, 2, (64, 64),d, 'exponential')\n",
    "        \n",
    "        # Generate testing images with different parameters\n",
    "        sphere_img_test = system_maker(20, 30, 1, 100, 2, (64, 64),d)\n",
    "        ellips_img_test = system_maker2(20, 30, 1, 100, 2,(64, 64),d, 'exponential')\n",
    "    \n",
    "        end = timer()\n",
    "        \n",
    "        print(f'\\nvariance a:{variance_a}, variance b:{variance_b}, mean: {mean}, covarient: {cov_ab}')\n",
    "        print(f\"Total generation time: {end-start:.3f} seconds\")\n",
    "        \n",
    "        # Concatenate the training images and labels\n",
    "        images_train = np.concatenate((sphere_img_train, ellips_img_train), axis=0)\n",
    "        labels_train = label_making(0, sphere_img_train) + label_making(1, ellips_img_train)\n",
    "        # plot_images(images_train[4],images_train[0],images_train[14],images_train[11],f'{image_name}_a{i}_b{j}')\n",
    "        plot_images(images_train[4],images_train[10],images_train[30],images_train[40],f'{image_name}_{mean}')\n",
    "        \n",
    "        # Concatenate the testing images and labels\n",
    "        images_test = np.concatenate((sphere_img_test, ellips_img_test), axis=0)\n",
    "        labels_test = label_making(0, sphere_img_test) + label_making(1, ellips_img_test)\n",
    "        \n",
    "        # Convert labels to numpy arrays\n",
    "        labels_array_train = np.array(labels_train)\n",
    "        labels_array_test = np.array(labels_test)\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        X_train, y_train = shuffle(images_train, labels_array_train, random_state=42)\n",
    "        X_val, y_val = shuffle(images_test, labels_array_test, random_state=42)\n",
    "        \n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\\n\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        # Instantiate the dataset\n",
    "        BATCH_SIZE = 32\n",
    "        NUM_WORKERS = 0 #os.cpu_count()\n",
    "        \n",
    "        train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=NUM_WORKERS)\n",
    "    \n",
    "        # Trainig begins\n",
    "        # Set no of epochs (newnet)\n",
    "        NUM_EPOCHS = 10\n",
    "    \n",
    "        # Setup loss function and optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "        # FIrst test  without training\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=val_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        print(f\" Model performance before training | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\\n\")\n",
    "\n",
    "        \n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "        \n",
    "        # Train model\n",
    "        model_results = train(model=model,\n",
    "                             train_data=train_dataloader,\n",
    "                             test_data=val_dataloader,\n",
    "                             optimizer=optimizer,\n",
    "                             loss_fn=loss_fn,\n",
    "                             epochs=NUM_EPOCHS)\n",
    "\n",
    "        \n",
    "        # End timer and print out time taken\n",
    "        end_time = timer()\n",
    "        print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "        \n",
    "        # Calculate parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f'Total parameters: {total_params}\\n')\n",
    "        plot_loss_curves(model_results, f'Loss_{mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5855b232-c798-420c-b662-a16fcea68646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9093453506445872 1.1780419422311963\n",
      "0.6560813065504778 6.216010313716924\n",
      "0.003821933799319916 1.9405053912048071\n",
      "0.22731100484794078 1.1496576503734475\n",
      "1.5025988033022164 2.3710444401720974\n",
      "3.027141932216918 0.9137305037679806\n",
      "2.544147850574235 0.6285614076703888\n",
      "2.469769913366465 2.134380633899582\n",
      "0.45810536060463614 0.6227810497898486\n",
      "1.490575454054746 3.9519900349108568\n"
     ]
    }
   ],
   "source": [
    "mean = [1, 2]  # Parameters for the distributions\n",
    "variance_a = 0.1  # Variance for a/c (used for Gaussian)\n",
    "variance_b = 0.1  # Variance for b/c (used for Gaussian)\n",
    "cov_ab = 0.5  # Covariance between a/c and b/c (used for Gaussian)\n",
    "\n",
    "# Covariance matrix for the bivariate normal distribution\n",
    "cov = [[variance_a, cov_ab], \n",
    "       [cov_ab, variance_b]]\n",
    "for _ in range(10):\n",
    "    a, b = generate_ratios(mean, cov, \"exponential\")\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba284b-119b-4e59-9fdf-8c138f7a0691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
